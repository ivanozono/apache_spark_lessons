{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c9064e",
   "metadata": {},
   "source": [
    "## Introducción a MLlib\n",
    "\n",
    "Apache Spark MLlib es la biblioteca de aprendizaje automático de Spark. Proporciona múltiples herramientas y algoritmos para el diseño y entrenamiento de modelos, así como para su evaluación y despliegue. MLlib soporta diferentes tipos de algoritmos de aprendizaje automático, incluyendo clasificación, regresión, clustering y filtrado colaborativo, entre otros. Además, ofrece herramientas para la construcción de pipelines de aprendizaje automático, lo que facilita el proceso de construcción, evaluación y despliegue de modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a85d9",
   "metadata": {},
   "source": [
    "## Características principales de MLlib\n",
    "\n",
    "Las capacidades de MLlib se pueden agrupar en varias categorías:\n",
    "\n",
    "1. **Algoritmos de aprendizaje automático**: Esto incluye herramientas comunes para clasificación, regresión, clustering y filtrado colaborativo.\n",
    "2. **Transformaciones de features**: Herramientas para transformar y procesar características. Por ejemplo, codificación one-hot, normalización, etc.\n",
    "3. **Pipelines**: Herramientas para construir, evaluar y ajustar pipelines de aprendizaje automático.\n",
    "4. **Almacenamiento y exportación de modelos**: Una vez que un modelo ha sido entrenado, puede ser almacenado y exportado para su uso en otros sistemas o aplicaciones.\n",
    "5. **Herramientas de evaluación**: Métodos para evaluar la eficacia de los modelos en función de ciertos criterios, como la precisión, el AUC, entre otros.\n",
    "6. **Utilidades de álgebra lineal y estadísticas**: Funciones útiles para trabajar con datos y realizar operaciones matemáticas y estadísticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f883a",
   "metadata": {},
   "source": [
    "## Ventajas de usar MLlib\n",
    "\n",
    "1. **Escalabilidad**: Puede manejar grandes conjuntos de datos aprovechando la naturaleza distribuida de Spark.\n",
    "2. **Integración con Spark**: Al ser una biblioteca nativa de Spark, se integra perfectamente con otras herramientas y bibliotecas de Spark, como Spark SQL y Spark Streaming.\n",
    "3. **Variedad de algoritmos**: Ofrece una amplia gama de algoritmos de aprendizaje automático, desde clasificación hasta recomendación.\n",
    "4. **Flexibilidad**: Proporciona herramientas para construir pipelines completos de aprendizaje automático, desde el preprocesamiento de datos hasta la evaluación del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03051c9",
   "metadata": {},
   "source": [
    "## DataFrames y Datasets en MLlib\n",
    "\n",
    "MLlib utiliza DataFrames y Datasets como estructuras de datos principales. Estas estructuras permiten un procesamiento eficiente y distribuido de los datos. Además, proporcionan una API unificada para trabajar con datos, ya sea para tareas de procesamiento o para entrenamiento de modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb6e8e",
   "metadata": {},
   "source": [
    "## Transformers y Estimators\n",
    "\n",
    "En MLlib, un **Transformer** es un algoritmo que puede transformar un DataFrame en otro, generalmente añadiendo una o más columnas. Por ejemplo, un modelo de aprendizaje automático es un Transformer que añade predicciones a un DataFrame.\n",
    "\n",
    "Un **Estimator** es un algoritmo que se puede ajustar a un DataFrame para producir un Transformer. Por ejemplo, un algoritmo de aprendizaje es un Estimator que se entrena en un DataFrame y produce un modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b8823",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Un pipeline encadena múltiples etapas de Transformers y Estimators para especificar un flujo de trabajo de aprendizaje automático. Es una forma de organizar y estructurar el proceso de construcción, entrenamiento y evaluación de modelos en Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36b777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las bibliotecas necesarias de MLlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Configuración básica (aquí sólo estamos importando, no hay salida específica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4a914",
   "metadata": {},
   "source": [
    "En la celda anterior, hemos importado algunas de las herramientas básicas que MLlib nos proporciona. Estas herramientas se utilizarán en ejemplos posteriores. Es esencial familiarizarse con los módulos y las clases que ofrece MLlib, ya que la construcción de modelos y pipelines suele requerir la combinación de múltiples herramientas y algoritmos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c63cb3",
   "metadata": {},
   "source": [
    "## Algoritmos de aprendizaje automático en Spark\n",
    "\n",
    "MLlib proporciona implementaciones de algoritmos de aprendizaje automático que son escalables y diseñados para trabajar con datos distribuidos. Estos algoritmos cubren una variedad de tareas, desde regresión y clasificación hasta clustering y reducción de dimensionalidad. Vamos a explorar algunos de estos algoritmos y cómo se pueden utilizar en Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea0c5c",
   "metadata": {},
   "source": [
    "## Inicialización de SparkSession\n",
    "\n",
    "Para trabajar con Spark MLlib, primero debemos inicializar una SparkSession. Esta SparkSession actuará como el punto de entrada para nuestras operaciones con Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ae3103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 18:06:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark MLlib Intro\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96153a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626d165f",
   "metadata": {},
   "source": [
    "### Regresión Lineal\n",
    "\n",
    "Uno de los algoritmos más básicos y ampliamente utilizados en aprendizaje supervisado es la regresión lineal. Es utilizado para predecir un valor numérico basado en variables independientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfc6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 11:16:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.7453384173419662], Intercept: 1.381992373987051\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [\n",
    "    (Vectors.dense([0.0]), 1.0),\n",
    "    (Vectors.dense([1.0]), 2.0),\n",
    "    (Vectors.dense([2.0]), 3.0),\n",
    "    (Vectors.dense([3.0]), 4.0)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"features\", \"label\"])\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(df)\n",
    "\n",
    "# Mostrar los coeficientes y la ordenada al origen\n",
    "print(f\"Coefficients: {lr_model.coefficients}, Intercept: {lr_model.intercept}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a6e4a",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, hemos creado un pequeño conjunto de datos para regresión lineal y entrenado un modelo utilizando MLlib. La regresión lineal intenta encontrar la mejor línea (en términos de mínimos cuadrados) que se ajuste a los datos. Los resultados mostrados son los coeficientes de las variables independientes y la ordenada al origen de la línea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3605d71",
   "metadata": {},
   "source": [
    "## Algoritmos de clasificación\n",
    "\n",
    "La clasificación es una técnica de aprendizaje supervisado que tiene como objetivo categorizar las entradas en clases distintas. En Spark MLlib, hay varios algoritmos de clasificación disponibles, como regresión logística, árboles de decisión, máquinas de vectores de soporte, entre otros. En este segmento, exploraremos cómo implementar la regresión logística usando Spark MLlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5784c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 11:25:11 WARN Instrumentation: [fb891952] All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------+----------+\n",
      "|      features|label|probability|prediction|\n",
      "+--------------+-----+-----------+----------+\n",
      "|[2.0,1.0,-1.0]|  0.0|  [0.0,1.0]|       1.0|\n",
      "| [2.0,1.3,1.0]|  0.0|  [0.0,1.0]|       1.0|\n",
      "+--------------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Creación de un conjunto de datos de muestra\n",
    "sample_data = [\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sample_data, [\"label\", \"features\"])\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "# Definir el modelo de regresión logística\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# Predicciones\n",
    "predictions = lr_model.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d17e01",
   "metadata": {},
   "source": [
    "En el código anterior:\n",
    "\n",
    "1. Importamos las bibliotecas necesarias para la regresión logística y la creación de vectores.\n",
    "2. Creamos un conjunto de datos de muestra con etiquetas y características.\n",
    "3. Dividimos los datos en conjuntos de entrenamiento y prueba.\n",
    "4. Definimos el modelo de regresión logística con un máximo de 10 iteraciones y un parámetro de regularización de 0.01.\n",
    "5. Entrenamos el modelo usando el conjunto de entrenamiento.\n",
    "6. Realizamos predicciones en el conjunto de prueba y mostramos las características, la etiqueta real, la probabilidad y la predicción.\n",
    "\n",
    "En escenarios reales, el proceso involucra más pasos, como la selección de características, el ajuste de hiperparámetros y la evaluación del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a9483",
   "metadata": {},
   "source": [
    "## Árboles de Decisión\n",
    "\n",
    "Los árboles de decisión son uno de los algoritmos más intuitivos y populares en aprendizaje automático. Son estructuras de árbol donde cada nodo representa una característica (o atributo), cada enlace (o rama) representa una decisión (regla) sobre la característica, y cada hoja representa un resultado (categoría o valor continuo, dependiendo de si es clasificación o regresión).\n",
    "\n",
    "Spark MLlib proporciona un implementación eficiente de árboles de decisión para clasificación y regresión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166bd2c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Vamos a trabajar con un conjunto de datos simulado sobre la aprobación de préstamos. Las características incluyen:\n",
    "\n",
    "- Edad: la edad del solicitante.\n",
    "- Salario: el salario mensual del solicitante.\n",
    "- Historial Crediticio: un valor entre 0 (mal historial) y 1 (buen historial).\n",
    "\n",
    "La etiqueta es:\n",
    "\n",
    "- Aprobado: 1 si el préstamo fue aprobado, 0 si fue rechazado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2b87d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 18:17:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|          features|aprobado|\n",
      "+------------------+--------+\n",
      "| [25.0,5000.0,0.6]|       0|\n",
      "| [45.0,8000.0,0.8]|       1|\n",
      "| [35.0,6000.0,0.9]|       1|\n",
      "| [50.0,9000.0,0.4]|       0|\n",
      "| [23.0,4000.0,0.7]|       0|\n",
      "|[55.0,12000.0,0.9]|       1|\n",
      "| [30.0,4500.0,0.5]|       0|\n",
      "| [40.0,7500.0,0.8]|       1|\n",
      "|[60.0,10000.0,0.6]|       0|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder.appName(\"LoanApproval\").getOrCreate()\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [\n",
    "    (25, 5000, 0.6, 0),\n",
    "    (45, 8000, 0.8, 1),\n",
    "    (35, 6000, 0.9, 1),\n",
    "    (50, 9000, 0.4, 0),\n",
    "    (23, 4000, 0.7, 0),\n",
    "    (55, 12000, 0.9, 1),\n",
    "    (30, 4500, 0.5, 0),\n",
    "    (40, 7500, 0.8, 1),\n",
    "    (60, 10000, 0.6, 0)\n",
    "]\n",
    "\n",
    "# Crear DataFrame\n",
    "df = spark.createDataFrame(data, [\"edad\", \"salario\", \"historial_crediticio\", \"aprobado\"])\n",
    "\n",
    "# Convertir las características a un vector\n",
    "feature_cols = [\"edad\", \"salario\", \"historial_crediticio\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df.select(\"features\", \"aprobado\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9575ba6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Antes de entrenar el modelo, dividiremos los datos en un conjunto de entrenamiento y un conjunto de prueba. Esto nos permitirá evaluar el rendimiento del modelo en datos no vistos previamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ebf85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de ejemplos en el conjunto de entrenamiento: 6\n",
      "Número de ejemplos en el conjunto de prueba: 3\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Mostrar el tamaño de cada conjunto\n",
    "print(f\"Número de ejemplos en el conjunto de entrenamiento: {train.count()}\")\n",
    "print(f\"Número de ejemplos en el conjunto de prueba: {test.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942e03e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Utilizaremos el conjunto de entrenamiento para entrenar un modelo de árbol de decisión y predecir si un préstamo será aprobado o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1c9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 18:20:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 6 (= number of training instances)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convertir las etiquetas en índices numéricos\n",
    "label_indexer = StringIndexer(inputCol=\"aprobado\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "# Crear el modelo de árbol de decisión\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "dt_model = dt.fit(label_indexer.transform(train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302ffcb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Con el modelo entrenado, ahora podemos hacer predicciones en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169bc46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+----------+\n",
      "|         features|aprobado|prediction|\n",
      "+-----------------+--------+----------+\n",
      "|[45.0,8000.0,0.8]|       1|       1.0|\n",
      "|[35.0,6000.0,0.9]|       1|       1.0|\n",
      "|[50.0,9000.0,0.4]|       0|       0.0|\n",
      "+-----------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "predictions = dt_model.transform(label_indexer.transform(test))\n",
    "\n",
    "# Mostrar las predicciones\n",
    "predictions.select(\"features\", \"aprobado\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3ca55",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest es un algoritmo de aprendizaje automático supervisado que se utiliza tanto para clasificación como para regresión. Es un conjunto de árboles de decisión, generalmente entrenados con el método de \"bagging\". La idea básica detrás del bagging es combinar los resultados de múltiples modelos (en este caso, árboles de decisión) para obtener una generalización final más fuerte y más estable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127b297",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Vamos a utilizar un conjunto de datos de ejemplo para predecir si una persona compra o no un producto en función de características como la edad y el salario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d407503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Conjunto de datos de ejemplo\n",
    "data_rf = [\n",
    "    (20, 40000, 0),\n",
    "    (22, 41000, 0),\n",
    "    (28, 60000, 1),\n",
    "    (32, 65000, 1),\n",
    "    (40, 80000, 0)\n",
    "]\n",
    "columns_rf = [\"edad\", \"salario\", \"compra\"]\n",
    "df_rf = spark.createDataFrame(data_rf, schema=columns_rf)\n",
    "\n",
    "# Convertir las etiquetas en índices numéricos\n",
    "label_indexer_rf = StringIndexer(inputCol=\"compra\", outputCol=\"indexedLabel\").fit(df_rf)\n",
    "\n",
    "# Convertir las columnas en un vector de características\n",
    "vector_assembler_rf = VectorAssembler(inputCols=[\"edad\", \"salario\"], outputCol=\"features\")\n",
    "df_rf = vector_assembler_rf.transform(df_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2077e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 18:32:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-----------+----------+\n",
      "|edad|salario|compra|probability|prediction|\n",
      "+----+-------+------+-----------+----------+\n",
      "|  20|  40000|     0|  [0.7,0.3]|       0.0|\n",
      "|  22|  41000|     0|  [0.6,0.4]|       0.0|\n",
      "|  28|  60000|     1|  [0.1,0.9]|       1.0|\n",
      "|  32|  65000|     1|  [0.2,0.8]|       1.0|\n",
      "|  40|  80000|     0|  [0.8,0.2]|       0.0|\n",
      "+----+-------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n",
    "rf_model = rf.fit(label_indexer_rf.transform(df_rf))\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions_rf = rf_model.transform(label_indexer_rf.transform(df_rf))\n",
    "predictions_rf.select(\"edad\", \"salario\", \"compra\", \"probability\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b40864",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "El clustering, o agrupación, es un método de aprendizaje automático no supervisado que se utiliza para dividir un conjunto de datos en grupos o \"clusters\". Los datos dentro de un cluster son más similares entre sí que con los de otros clusters. Uno de los algoritmos de clustering más conocidos es el K-means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225dea9",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Vamos a utilizar el algoritmo K-means para agrupar datos de ejemplo basados en características dadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47459c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Primero, creamos un conjunto de datos de ejemplo que consta de la \"edad\" y el \"salario\" de individuos. Este conjunto de datos se utilizará para demostrar cómo el algoritmo K-means puede agrupar datos basados en estas características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ecf6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Conjunto de datos de ejemplo\n",
    "data_kmeans = [\n",
    "    (20, 40000),\n",
    "    (22, 41000),\n",
    "    (28, 60000),\n",
    "    (32, 65000),\n",
    "    (40, 80000),\n",
    "    (42, 82000),\n",
    "    (48, 99000),\n",
    "    (50, 101000)\n",
    "]\n",
    "columns_kmeans = [\"edad\", \"salario\"]\n",
    "df_kmeans = spark.createDataFrame(data_kmeans, schema=columns_kmeans)\n",
    "\n",
    "# Convertir las columnas en un vector de características\n",
    "vector_assembler_kmeans = VectorAssembler(inputCols=[\"edad\", \"salario\"], outputCol=\"features\")\n",
    "df_kmeans = vector_assembler_kmeans.transform(df_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e7cdf",
   "metadata": {},
   "source": [
    "VectorAssembler\n",
    "\n",
    "Antes de poder usar muchos de los algoritmos de MLlib, necesitamos convertir nuestras columnas de datos en una única columna de vectores. `VectorAssembler` es una transformación que combina una lista dada de columnas en una única columna de vectores. En este caso, combinamos las columnas \"edad\" y \"salario\" en una columna \"features\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b3b13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Aquí, inicializamos el algoritmo K-means con un valor específico de `k`, que representa el número de clusters que queremos. En este ejemplo, hemos elegido `k=2`, lo que significa que queremos dividir nuestros datos en dos clusters.\n",
    "\n",
    "Una vez inicializado el algoritmo, utilizamos el método `fit` para entrenar el modelo con nuestro conjunto de datos. El modelo entrenado calculará los centros de los clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a7386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 18:41:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centros de clusters:  [array([2.55e+01, 5.15e+04]), array([4.50e+01, 9.05e+04])]\n",
      "+----+-------+----------+\n",
      "|edad|salario|prediction|\n",
      "+----+-------+----------+\n",
      "|  20|  40000|         0|\n",
      "|  22|  41000|         0|\n",
      "|  28|  60000|         0|\n",
      "|  32|  65000|         0|\n",
      "|  40|  80000|         1|\n",
      "|  42|  82000|         1|\n",
      "|  48|  99000|         1|\n",
      "|  50| 101000|         1|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo K-means\n",
    "kmeans = KMeans(featuresCol=\"features\", k=2)\n",
    "kmeans_model = kmeans.fit(df_kmeans)\n",
    "\n",
    "# Obtener los centros de los clusters\n",
    "centers = kmeans_model.clusterCenters()\n",
    "print(\"Centros de clusters: \", centers)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions_kmeans = kmeans_model.transform(df_kmeans)\n",
    "predictions_kmeans.select(\"edad\", \"salario\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f4d5d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una vez que el modelo ha sido entrenado, podemos utilizarlo para asignar cada punto de datos a uno de los clusters. Esto se hace mediante el método `transform`, que añade una nueva columna \"prediction\" a nuestro DataFrame, indicando a qué cluster pertenece cada punto de datos.\n",
    "\n",
    "También se imprimen los centros de los clusters, que son los puntos centrales de cada cluster y representan un \"promedio\" de los puntos de datos dentro de ese cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ea3b6",
   "metadata": {},
   "source": [
    "El algoritmo K-means utiliza una técnica de agrupación basada en la distancia para determinar a qué clúster pertenece cada punto de datos. La idea central de K-means es minimizar la distancia entre los puntos dentro de un clúster y maximizar la distancia entre diferentes clústeres. Veamos cómo se determinan las predicciones en tu caso específico:\n",
    "\n",
    "1. **Inicialización de los centros de clústeres**: Al principio, K-means selecciona aleatoriamente \\( k \\) puntos de datos del conjunto de datos como centros iniciales de los clústeres (en tu caso \\( k=2 \\)). También hay otras técnicas de inicialización, pero la inicialización aleatoria es la más común.\n",
    "\n",
    "2. **Asignación a clústeres**: Una vez que se han inicializado los centros, cada punto de datos se asigna al clúster cuyo centro está más cerca. La \"cercanía\" se mide típicamente usando la distancia euclidiana, aunque hay otras métricas de distancia que se pueden usar.\n",
    "\n",
    "3. **Recálculo de los centros**: Después de asignar todos los puntos a clústeres, se recalculan los centros de los clústeres tomando el promedio de todos los puntos dentro de un clúster.\n",
    "\n",
    "4. **Iteración**: Los pasos 2 y 3 se repiten hasta que los centros de los clústeres ya no cambien significativamente entre iteraciones o hasta que se alcance un número máximo de iteraciones.\n",
    "\n",
    "En este caso específico:\n",
    "- El primer centro de clúster es aproximadamente [25.5, 51500]. \n",
    "- El segundo centro de clúster es aproximadamente [45, 90500].\n",
    "\n",
    "Por lo tanto, los puntos de datos más cercanos al primer centro (basados en la distancia euclidiana) se etiquetan como `0`, y los puntos más cercanos al segundo centro se etiquetan como `1`.\n",
    "\n",
    "Para tus datos:\n",
    "- Los puntos [20,40000], [22,41000], [28,60000], y [32,65000] están más cerca del primer centro, por lo que se les asigna `0`.\n",
    "- Los puntos [40,80000], [42,82000], [48,99000], y [50,101000] están más cerca del segundo centro, por lo que se les asigna `1`.\n",
    "\n",
    "Estas asignaciones se reflejan en la columna `prediction` de tu resultado. Es importante recordar que las etiquetas `0` y `1` no tienen un significado inherente en sí mismas; simplemente representan dos clústeres diferentes identificados por el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b765474",
   "metadata": {},
   "source": [
    "## Sistemas de recomendación\n",
    "\n",
    "Los sistemas de recomendación se utilizan para predecir el \"rating\" o preferencia que un usuario le daría a un ítem. Son ampliamente utilizados en servicios en línea como Netflix, Amazon y Spotify para recomendar contenido y productos a sus usuarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f20f0",
   "metadata": {},
   "source": [
    "### ALS (Alternating Least Squares)\n",
    "\n",
    "ALS es un algoritmo de factorización de matrices que se utiliza para llenar valores faltantes en matrices de usuario-ítem. Se basa en descomponer la matriz de usuario-ítem en dos matrices de \"factores latentes\" que se multiplican para obtener la matriz original. Spark MLlib utiliza ALS para entrenar un modelo de sistema de recomendación.\n",
    "\n",
    "\n",
    "¿Por qué \"Alternating\" y \"Least Squares\"?\n",
    "\n",
    "Alternating: El algoritmo funciona alternando entre la optimización de la matriz de usuarios manteniendo fija la matriz de ítems y viceversa.\n",
    "\n",
    "Least Squares: Durante la optimización, se utiliza el método de mínimos cuadrados para minimizar el error entre la matriz original y el producto de las dos matrices factorizadas.\n",
    "\n",
    "Proceso paso a paso en ALS:\n",
    "\n",
    "Inicialización: Se inicializan aleatoriamente las matrices de usuarios e ítems.\n",
    "\n",
    "Optimización alternante: Manteniendo fija la matriz de ítems, se optimiza la matriz de usuarios para minimizar el error. Luego, manteniendo fija la matriz de usuarios, se optimiza la matriz de ítems.\n",
    "\n",
    "Este proceso se repite hasta que el error converge a un mínimo o se alcanza un número máximo de iteraciones.\n",
    "\n",
    "Regularización:\n",
    "Para evitar el sobreajuste, ALS incluye un término de regularización en su función de optimización. El parámetro regParam en nuestro código controla la regularización. Cuanto mayor sea este valor, más fuerte será la regularización.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6df531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|item_id|rating|\n",
      "+-------+-------+------+\n",
      "|      0|      0|     4|\n",
      "|      0|      1|     2|\n",
      "|      1|      1|     3|\n",
      "|      1|      2|     4|\n",
      "|      2|      0|     1|\n",
      "|      2|      2|     5|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Datos de ejemplo: [user_id, item_id, rating]\n",
    "data = [\n",
    "    Row(user_id=0, item_id=0, rating=4),\n",
    "    Row(user_id=0, item_id=1, rating=2),\n",
    "    Row(user_id=1, item_id=1, rating=3),\n",
    "    Row(user_id=1, item_id=2, rating=4),\n",
    "    Row(user_id=2, item_id=0, rating=1),\n",
    "    Row(user_id=2, item_id=2, rating=5)\n",
    "]\n",
    "\n",
    "ratings_df = spark.createDataFrame(data)\n",
    "ratings_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7f01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 18:53:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|prediction|\n",
      "+-------+-------+----------+\n",
      "|      0|      2| 1.5773301|\n",
      "|      1|      0| 2.7646449|\n",
      "|      2|      1|  2.000145|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Definir y entrenar el modelo ALS\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\")\n",
    "model = als.fit(ratings_df)\n",
    "\n",
    "# Realizar predicciones\n",
    "test_data = spark.createDataFrame([\n",
    "    Row(user_id=0, item_id=2),\n",
    "    Row(user_id=1, item_id=0),\n",
    "    Row(user_id=2, item_id=1)\n",
    "])\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e91e7",
   "metadata": {},
   "source": [
    "Datos de entrada: Tenemos una matriz de usuario-ítem que contiene ratings de ciertos usuarios para ciertos ítems. Hay muchos valores faltantes, que son precisamente las calificaciones que queremos predecir.\n",
    "\n",
    "Inicialización: ALS inicializa aleatoriamente las matrices de usuarios e ítems.\n",
    "\n",
    "Optimización: Se alternan las optimizaciones de las matrices de usuarios e ítems hasta que se alcanzan 5 iteraciones (como se especificó con maxIter=5).\n",
    "\n",
    "Predicción: Una vez que tenemos nuestras matrices factorizadas, podemos usarlas para predecir ratings para combinaciones de usuario-ítem que no estaban en el conjunto original. Lo hacemos multiplicando las características del usuario por las características del ítem.\n",
    "\n",
    "Resultado: Las predicciones resultantes son nuestras estimaciones de cómo un usuario calificaría un ítem dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ea9aa",
   "metadata": {},
   "source": [
    " Estas predicciones pueden ser utilizadas para recomendar ítems a usuarios basados en sus preferencias previas.\n",
    " \n",
    "El verdadero poder de ALS radica en su capacidad para descubrir características latentes, es decir, características que no se nos proporcionan explícitamente pero que el algoritmo identifica como importantes para hacer predicciones precisas. Por ejemplo, en un sistema de recomendación de películas, las características latentes podrían relacionarse con géneros, actores populares, directores, etc., aunque no se nos proporcionen explícitamente estos detalles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6880f6",
   "metadata": {},
   "source": [
    "## Algoritmos de Reducción de Dimensionalidad\n",
    "\n",
    "La reducción de dimensionalidad es una técnica clave en el análisis y procesamiento de datos. Su objetivo principal es reducir el número de características (dimensiones) en un conjunto de datos manteniendo la mayor cantidad de información posible. Esto es útil tanto para la visualización de datos como para mejorar el rendimiento de algoritmos de aprendizaje automático, especialmente cuando se enfrentan a conjuntos de datos de alta dimensión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464a2dc",
   "metadata": {},
   "source": [
    "### PCA (Análisis de Componentes Principales)\n",
    "\n",
    "PCA es uno de los métodos más populares para la reducción de dimensionalidad. Intenta encontrar las direcciones en las cuales los datos varían más. Estas direcciones, llamadas componentes principales, son combinaciones lineales de las características originales y son ortogonales entre sí.\n",
    "\n",
    "\n",
    "El Análisis de Componentes Principales (PCA) es un método estadístico que permite simplificar la complejidad en conjuntos de datos espaciales transformando un gran conjunto de variables en un conjunto menor, conservando la mayor cantidad de información original en términos de variabilidad.\n",
    "\n",
    "La idea principal detrás del PCA es identificar patrones en los datos y detectar la correlación entre características. Si un fuerte nivel de correlación existe entre variables, la intentona es reducir la dimensionalidad. Esto se logra transformando las características originales a un nuevo conjunto de características, los llamados \"componentes principales\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b9768",
   "metadata": {},
   "source": [
    "Funcionamiento de PCA\n",
    "\n",
    "1. **Estandarización de los datos:** La estandarización es esencial para el funcionamiento correcto de PCA, ya que es sensible a las variaciones en la magnitud.\n",
    "2. **Obtención de los Eigenvectors y Eigenvalues:** A partir de la matriz de covarianza o la matriz de correlación o incluso la técnica de Singular Vector Decomposition.\n",
    "3. **Ordenar los Eigenvalues:** Y seleccionar los `k` eigenvectors que corresponden a los `k` eigenvalues más grandes donde `k` es el número de dimensiones del nuevo subespacio de características (`k ≤ n`).\n",
    "4. **Construir la matriz de proyección W a partir de los `k` eigenvectors seleccionados.**\n",
    "5. **Transformar el conjunto de datos original X a través de W para obtener un subconjunto de características k-dimensional Y.**\n",
    "\n",
    "El resultado es un conjunto de características de dimensión reducida que captura la mayor variabilidad en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52395f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2772bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|pca_features                                                |\n",
      "+------------------------------------------------------------+\n",
      "|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n",
      "|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n",
      "|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Conjunto de datos de ejemplo\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# PCA\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "result.select(\"pca_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576c528",
   "metadata": {},
   "source": [
    "El PCA intenta encontrar las direcciones en las cuales los datos varían más. Estas direcciones, llamadas componentes principales, son combinaciones lineales de las características originales y son ortogonales entre sí. \n",
    "\n",
    "En nuestro ejemplo, teníamos un conjunto de datos de 5 dimensiones y lo redujimos a 3 dimensiones usando PCA.\n",
    "\n",
    "Pasos que seguimos:\n",
    "\n",
    "1. **Estandarización de los datos:** Esto es crucial ya que PCA es sensible a las magnitudes.\n",
    "2. **Aplicación de PCA:** Usando Spark MLlib, aplicamos PCA para reducir la dimensionalidad.\n",
    "3. **Transformación de los datos originales:** Usando el modelo PCA entrenado, transformamos nuestro conjunto de datos original en un nuevo conjunto con dimensiones reducidas.\n",
    "\n",
    "El resultado fue una representación comprimida de nuestro conjunto de datos original que, en un contexto real, podría usarse para acelerar algoritmos de aprendizaje automático sin perder mucha información.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daaa74c",
   "metadata": {},
   "source": [
    "## Ejemplo de análisis de sentimiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63025b8e",
   "metadata": {},
   "source": [
    "El análisis de sentimiento es una técnica de procesamiento de lenguaje natural (NLP, por sus siglas en inglés) que tiene como objetivo determinar la tonalidad emocional detrás de una serie de palabras. Se utiliza para obtener una comprensión de las opiniones y emociones expresadas en menciones, comentarios o reseñas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6825dc0",
   "metadata": {},
   "source": [
    "### Proceso del análisis de sentimiento\n",
    "\n",
    "1. **Tokenización**: Es el proceso de convertir un texto en tokens, que son secuencias de caracteres individuales. En el contexto del análisis de sentimiento, generalmente se refiere a convertir una frase o párrafo en palabras individuales.\n",
    "\n",
    "2. **Vectorización**: Una vez que tenemos nuestro texto tokenizado, el siguiente paso es convertir estos tokens en vectores de números que las máquinas pueden usar para aprender. Una técnica común es TF-IDF.\n",
    "\n",
    "3. **Modelo de clasificación**: Una vez que hemos vectorizado nuestro texto, podemos alimentar esos vectores en un modelo de clasificación, como la regresión logística, para predecir el sentimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2155e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|El producto es ex...|    1|\n",
      "|No funcionó corre...|    0|\n",
      "|El servicio al cl...|    1|\n",
      "|La calidad es pésima|    0|\n",
      "|El envío fue rápi...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [\n",
    "    Row(text=\"El producto es excelente, muy satisfecho\", label=1),\n",
    "    Row(text=\"No funcionó correctamente, muy decepcionado\", label=0),\n",
    "    Row(text=\"El servicio al cliente fue muy útil\", label=1),\n",
    "    Row(text=\"La calidad es pésima\", label=0),\n",
    "    Row(text=\"El envío fue rápido y el producto llegó en perfecto estado\", label=1)\n",
    "]\n",
    "\n",
    "# Convertir a DataFrame\n",
    "sentiment_data = spark.createDataFrame(data)\n",
    "sentiment_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "480b7082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 19:34:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/25 19:34:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                text|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|El producto es ex...|[6.71957055098081...|       1.0|\n",
      "|No funcionó corre...|[0.99999999815441...|       0.0|\n",
      "|El servicio al cl...|[4.64719650625665...|       1.0|\n",
      "|La calidad es pésima|[0.99999999764744...|       0.0|\n",
      "|El envío fue rápi...|[2.82353601867221...|       1.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Código: Ejemplo de análisis de sentimiento con Spark\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Suponemos que 'sentiment_data' es un DataFrame con las columnas 'text' (el texto de la opinión) y 'label' (0 para negativo, 1 para positivo).\n",
    "\n",
    "# 1. Tokenización\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(sentiment_data)\n",
    "\n",
    "# 2. Vectorización TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "tf_data = hashing_tf.transform(words_data)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "tf_idf_model = idf.fit(tf_data)\n",
    "tf_idf_data = tf_idf_model.transform(tf_data)\n",
    "\n",
    "# 3. Crear y entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(tf_idf_data)\n",
    "\n",
    "# Predicciones\n",
    "predictions = lr_model.transform(tf_idf_data)\n",
    "predictions.select(\"text\", \"probability\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc6719",
   "metadata": {},
   "source": [
    "Las opiniones \"El producto es excelente, muy satisfecho\", \"El servicio al cliente fue muy útil\" y \"El envío fue rápido y el producto llegó en perfecto estado\" fueron clasificadas correctamente como positivas por el modelo.\n",
    "\n",
    "Las opiniones \"No funcionó correctamente, muy decepcionado\" y \"La calidad es pésima\" fueron identificadas correctamente como negativas.\n",
    "\n",
    "Esto sugiere que nuestro modelo hizo un buen trabajo al clasificar estas opiniones según el sentimiento expresado en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b3b04",
   "metadata": {},
   "source": [
    "## Ejemplo de clasificación de texto\n",
    "\n",
    "La clasificación de texto es una técnica de procesamiento del lenguaje natural y aprendizaje automático que etiqueta o categoriza el texto según su contenido. Es uno de los ejemplos más comunes de aplicaciones de NLP y se utiliza en una variedad de tareas, desde organización de documentos hasta análisis de sentimiento.\n",
    "\n",
    "### Uso de Naive Bayes para clasificación de texto\n",
    "\n",
    "Naive Bayes es un algoritmo basado en el teorema de Bayes con supuestos de independencia entre los predictores. En términos simples, un clasificador Naive Bayes asume que la presencia de una característica particular en una clase no está relacionada con la presencia de ninguna otra característica.\n",
    "\n",
    "Paso 1: Crear un conjunto de datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b73b3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Ejemplo de DataFrame con texto y etiquetas\n",
    "data = [\n",
    "    (0, \"Python Spark MLlib\"),\n",
    "    (0, \"Python Spark\"),\n",
    "    (1, \"Big data Hadoop\"),\n",
    "    (1, \"Spark vs Hadoop\"),\n",
    "    (2, \"Machine learning AI\"),\n",
    "    (2, \"Spark MLlib AI\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"label\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f3e50",
   "metadata": {},
   "source": [
    "Paso 2: Preprocesamiento y tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "667db445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tokenización del texto\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df)\n",
    "\n",
    "# Vectorización usando CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "model = cv.fit(words_data)\n",
    "count_vectorized = model.transform(words_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79091f58",
   "metadata": {},
   "source": [
    "Paso 3: Entrenar el modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de2ad2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "(train, test) = count_vectorized.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Usar Naive Bayes para entrenar el modelo\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"label\")\n",
    "modelo = nb.fit(train)\n",
    "\n",
    "# Realizar predicciones\n",
    "predicciones = modelo.transform(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549e823",
   "metadata": {},
   "source": [
    "Paso 4: Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d75d0014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluador = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluador.evaluate(predicciones)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7dc8a8",
   "metadata": {},
   "source": [
    "La tokenización es el proceso de convertir el texto en tokens o palabras individuales.\n",
    "\n",
    "`CountVectorizer` convierte una colección de documentos de texto en vectores de recuentos de tokens.\n",
    "\n",
    "El modelo Naive Bayes es sencillo y eficiente para tareas de clasificación de texto, especialmente cuando el conjunto de datos es grande.\n",
    "\n",
    "El evaluador `MulticlassClassificationEvaluator` nos proporciona una métrica (en este caso, precisión) para evaluar la calidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9430938",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "En este caso, el modelo de clasificación de texto basado en Naive Bayes logró una precisión de \n",
    "1.00\n",
    "1.00 o \n",
    "100\n",
    "%\n",
    "100%. Esto significa que el modelo predijo correctamente la etiqueta o categoría para todas las observaciones en el conjunto de datos de prueba.\n",
    "\n",
    "Interpretación:\n",
    "Un valor de precisión de \n",
    "1.00\n",
    "1.00 indica que el modelo realizó predicciones perfectas en el conjunto de datos de prueba. Sin embargo, es importante tener precaución con este resultado:\n",
    "\n",
    "Si el conjunto de datos es pequeño o no es representativo, un valor de precisión alto podría no reflejar el rendimiento real del modelo en datos no vistos.\n",
    "Una precisión del \n",
    "100\n",
    "%\n",
    "100% puede ser indicativa de un ajuste excesivo (overfitting), donde el modelo podría estar demasiado adaptado a los datos de entrenamiento y no generalizar bien a nuevos datos.\n",
    "Siempre es recomendable validar el modelo con diferentes conjuntos de datos y utilizar múltiples métricas para obtener una visión completa del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86c462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bfc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6a6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdaf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9cc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acaaa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e5e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87222ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b86abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fab6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e562c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac571cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393468b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961abbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86696a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d51b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9604b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1211d3e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58125043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fc308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56190aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c0e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37ee82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78696faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9009192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe320d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc6fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ae6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
