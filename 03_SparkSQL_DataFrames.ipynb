{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cda933e",
   "metadata": {},
   "source": [
    "# Introducción a Spark SQL y DataFrames\n",
    "\n",
    "Spark SQL es un módulo de Spark que permite el procesamiento estructurado de datos. A través de Spark SQL, puedes ejecutar consultas SQL directamente sobre tus datos en Spark, y también puedes leer datos de diversas fuentes estructuradas como Hive, Avro, Parquet, ORC, JSON y JDBC. \n",
    "\n",
    "Los `DataFrames` en Spark representan una tabla estructurada de datos. Es similar a un DataFrame en R o en Python con Pandas, pero con optimizaciones para ejecución distribuida y escalable. Puedes pensar en un DataFrame como una hoja de cálculo distribuida.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533d5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/21 21:04:38 WARN Utils: Your hostname, MacBook-Air-de-Ivan.local resolves to a loopback address: 127.0.0.1; using 192.168.0.2 instead (on interface en0)\n",
      "23/09/21 21:04:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/21 21:04:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IntroSparkSQL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd9c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0482b278",
   "metadata": {},
   "source": [
    "Una vez que hayas importado las bibliotecas necesarias, el siguiente paso es crear una `SparkSession`. La `SparkSession` es una interfaz para trabajar con datos estructurados en Spark. Puedes pensar en esto como una conexión a Spark. El parámetro `appName` simplemente da un nombre a tu aplicación para que puedas identificarla en la UI de Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e9bf6",
   "metadata": {},
   "source": [
    "## Operaciones Básicas con DataFrames\n",
    "\n",
    "Una vez que tengas un DataFrame, hay muchas operaciones que puedes realizar con él. Estas operaciones se pueden categorizar en:\n",
    "\n",
    "1. **Transformaciones:** Estas operaciones crean un nuevo DataFrame a partir de uno existente, como seleccionar ciertas columnas o filtrar filas basadas en una condición.\n",
    "2. **Acciones:** Estas operaciones devuelven un valor al controlador o escriben datos a un almacenamiento externo. Ejemplos incluyen contar el número de filas o escribir el DataFrame a un archivo.\n",
    "\n",
    "A continuación, veremos algunas operaciones básicas que te ayudarán a familiarizarte con los DataFrames en Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e141bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+\n",
      "|first_name|last_name|country|age|\n",
      "+----------+---------+-------+---+\n",
      "|     James|    Smith|    USA| 34|\n",
      "|   Michael|     Rose|    USA| 40|\n",
      "|    Robert| Williams| Canada| 37|\n",
      "|     Maria|    Jones| Mexico| 27|\n",
      "+----------+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = [(\"James\", \"Smith\", \"USA\", 34),\n",
    "        (\"Michael\", \"Rose\", \"USA\", 40),\n",
    "        (\"Robert\", \"Williams\", \"Canada\", 37),\n",
    "        (\"Maria\", \"Jones\", \"Mexico\", 27)\n",
    "       ]\n",
    "\n",
    "columns = [\"first_name\", \"last_name\", \"country\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052e1a5",
   "metadata": {},
   "source": [
    "En el código anterior, creamos un DataFrame llamado `df` usando datos de ejemplo. Las columnas son \"first_name\", \"last_name\", \"country\" y \"age\". La función `show()` nos permite visualizar el contenido del DataFrame. \n",
    "\n",
    "Es importante notar que Spark opera en modo \"lazy\", lo que significa que las transformaciones no se ejecutan hasta que se llama a una acción. En este caso, la acción es `show()`, que desencadena la ejecución y muestra el resultado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f42863",
   "metadata": {},
   "source": [
    "## Selección de Columnas\n",
    "\n",
    "Una de las operaciones más comunes que podrías querer realizar con un DataFrame es seleccionar una o más columnas. Esto se puede hacer utilizando la función `select`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|first_name|country|\n",
      "+----------+-------+\n",
      "|     James|    USA|\n",
      "|   Michael|    USA|\n",
      "|    Robert| Canada|\n",
      "|     Maria| Mexico|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar columnas first_name y country\n",
    "selected_df = df.select(\"first_name\", \"country\")\n",
    "selected_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c377b6f",
   "metadata": {},
   "source": [
    "En el código anterior, seleccionamos solo las columnas \"first_name\" y \"country\" del DataFrame original `df`. El resultado es un nuevo DataFrame, `selected_df`, que contiene solo estas dos columnas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e30d9",
   "metadata": {},
   "source": [
    "## Filtrado de Filas\n",
    "\n",
    "Otra operación común es filtrar filas basadas en ciertas condiciones. Esto se puede hacer utilizando la función `filter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcc77dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+\n",
      "|first_name|last_name|country|age|\n",
      "+----------+---------+-------+---+\n",
      "|     James|    Smith|    USA| 34|\n",
      "|   Michael|     Rose|    USA| 40|\n",
      "+----------+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar las filas donde el país es \"USA\"\n",
    "usa_df = df.filter(df.country == \"USA\")\n",
    "usa_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1dc08",
   "metadata": {},
   "source": [
    "En el código anterior, filtramos las filas donde la columna \"country\" es igual a \"USA\". El resultado, `usa_df`, es un nuevo DataFrame que contiene solo las filas que cumplen con esta condición.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85df2c6",
   "metadata": {},
   "source": [
    "## Agregación\n",
    "\n",
    "La agregación se refiere a cualquier operación que tome múltiples filas y las produzca como una sola fila de salida. Spark SQL proporciona funciones integradas para realizar agregaciones, como `count`, `sum`, `max`, `min`, entre otras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32728974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|    USA|    2|\n",
      "| Canada|    1|\n",
      "| Mexico|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Contar el número de personas por país\n",
    "count_by_country = df.groupBy(\"country\").count()\n",
    "count_by_country.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1cbfd",
   "metadata": {},
   "source": [
    "En el código anterior, agrupamos el DataFrame por la columna \"country\" y luego contamos el número de registros para cada país. El resultado, `count_by_country`, muestra el número de personas por país.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66282d01",
   "metadata": {},
   "source": [
    "## Join (Unión)\n",
    "\n",
    "El join es una operación que combina filas de dos o más tablas basadas en columnas relacionadas. Esto es similar a los \"joins\" en SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0ae4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 8) / 8]\r",
      "\r",
      "[Stage 12:=============>    (6 + 2) / 8][Stage 13:>                 (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+----+-------------+\n",
      "|first_name|last_name|country|age|code|    full_name|\n",
      "+----------+---------+-------+---+----+-------------+\n",
      "|     James|    Smith|    USA| 34| USA|United States|\n",
      "|   Michael|     Rose|    USA| 40| USA|United States|\n",
      "+----------+---------+-------+---+----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Suponiendo que tenemos un segundo DataFrame con la información de los códigos de los países\n",
    "country_codes_df = spark.createDataFrame([(\"USA\", \"United States\"), (\"MEX\", \"Mexico\"), (\"CAN\", \"Canada\")], [\"code\", \"full_name\"])\n",
    "\n",
    "# Realizamos un join basado en la columna 'country'\n",
    "joined_df = df.join(country_codes_df, df.country == country_codes_df.code)\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aef9be",
   "metadata": {},
   "source": [
    "En el código anterior, tenemos un DataFrame adicional `country_codes_df` que mapea códigos de país a sus nombres completos. Luego, realizamos una operación de join entre `df` y `country_codes_df` basado en la columna \"country\". Esto nos da un DataFrame `joined_df` que tiene la información combinada de ambos DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e86d8",
   "metadata": {},
   "source": [
    "## Ordenación de DataFrames\n",
    "\n",
    "Una tarea común al trabajar con DataFrames es ordenar los datos según uno o más criterios. Spark proporciona el método `orderBy` para ordenar los datos en un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857e9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+\n",
      "|first_name|last_name|country|age|\n",
      "+----------+---------+-------+---+\n",
      "|   Michael|     Rose|    USA| 40|\n",
      "|    Robert| Williams| Canada| 37|\n",
      "|     James|    Smith|    USA| 34|\n",
      "|     Maria|    Jones| Mexico| 27|\n",
      "+----------+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenar el DataFrame por la columna 'age' en orden descendente\n",
    "sorted_df = df.orderBy(df.age.desc())\n",
    "sorted_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4b6e2",
   "metadata": {},
   "source": [
    "En el código mostrado, estamos ordenando el DataFrame `df` por la columna 'age' en orden descendente. La función `desc()` se utiliza para indicar el orden descendente. Si quisiéramos un orden ascendente, simplemente omitiríamos `desc()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87be07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+\n",
      "|first_name|last_name|country|age|\n",
      "+----------+---------+-------+---+\n",
      "|   Michael|     Rose|    USA| 40|\n",
      "|    Robert| Williams| Canada| 37|\n",
      "|     James|    Smith|    USA| 34|\n",
      "|     Maria|    Jones| Mexico| 27|\n",
      "+----------+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenar el DataFrame 'df' por la columna 'age' en orden descendente\n",
    "sorted_df = df.orderBy(F.desc(\"age\"))\n",
    "sorted_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418eaf8a",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, hemos ordenado el DataFrame `df` basado en la columna 'age' en orden descendente. Para ello, utilizamos la función `desc` del módulo `functions` para especificar el orden descendente. El resultado muestra el DataFrame con los registros ordenados por edad, desde el más viejo al más joven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af592c",
   "metadata": {},
   "source": [
    "## Grouping y Aggregations Avanzadas\n",
    "\n",
    "Más allá de contar registros, Spark permite realizar múltiples agregaciones después de agrupar. Esto es útil para obtener estadísticas descriptivas por grupo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6258d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|country|average_age|max_age|\n",
      "+-------+-----------+-------+\n",
      "|    USA|       37.0|     40|\n",
      "| Canada|       37.0|     37|\n",
      "| Mexico|       27.0|     27|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agrupar por 'country' y obtener el promedio y el máximo de edad\n",
    "grouped_data = df.groupBy(\"country\").agg(F.avg(\"age\").alias(\"average_age\"), F.max(\"age\").alias(\"max_age\"))\n",
    "grouped_data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6bc27",
   "metadata": {},
   "source": [
    "En el código anterior, después de agrupar el DataFrame `df` por la columna 'country', aplicamos dos funciones de agregación: el promedio y el máximo de la columna 'age'. Usamos `alias` para dar nombres más descriptivos a las columnas resultantes. El resultado es un DataFrame con el promedio y el máximo de edad por país.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10789c7d",
   "metadata": {},
   "source": [
    "## Creación de DataFrames en Spark\n",
    "\n",
    "En Spark, un DataFrame es una estructura de datos distribuida organizada en columnas nombradas. Es equivalente a una tabla en una base de datos o un dataframe en R o Python, pero con optimizaciones para el procesamiento en memoria y escalabilidad para big data. Se pueden crear DataFrames en Spark de diversas formas:\n",
    "\n",
    "1. A partir de RDDs.\n",
    "2. Desde estructuras de datos en Python, como listas y diccionarios.\n",
    "3. Leyendo desde archivos (CSV, Parquet, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7332fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    Alice| 34|\n",
      "|      Bob| 45|\n",
      "|Catherine| 29|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Crear un DataFrame a partir de una lista\n",
    "list_data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
    "rdd = spark.sparkContext.parallelize(list_data)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=x[1]))\n",
    "df_from_list = spark.createDataFrame(people)\n",
    "df_from_list.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7ce5f",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, comenzamos con una lista simple de tuples en Python. Convertimos esta lista en un RDD usando `sparkContext.parallelize`. Luego, transformamos este RDD en un conjunto de filas (usando `Row`) y finalmente creamos un DataFrame a partir de este RDD transformado. El resultado es un DataFrame con las columnas `name` y `age`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e58ab",
   "metadata": {},
   "source": [
    "## Creación de DataFrame a partir de diccionarios\n",
    "\n",
    "Podemos también crear DataFrames directamente desde diccionarios. Esta es una forma conveniente cuando ya tenemos datos estructurados en forma de diccionarios en Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976afef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|age|     name|\n",
      "+---+---------+\n",
      "| 34|    Alice|\n",
      "| 45|      Bob|\n",
      "| 29|Catherine|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de una lista de diccionarios\n",
    "data_list_of_dicts = [{\"name\": \"Alice\", \"age\": 34}, {\"name\": \"Bob\", \"age\": 45}, {\"name\": \"Catherine\", \"age\": 29}]\n",
    "df_from_list_of_dicts = spark.createDataFrame(data_list_of_dicts)\n",
    "df_from_list_of_dicts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076579bf",
   "metadata": {},
   "source": [
    "En el código anterior, tenemos una lista que contiene tres diccionarios. Cada diccionario representa una fila en nuestro DataFrame deseado. Utilizamos `spark.createDataFrame()` para convertir esta lista de diccionarios en un DataFrame. El resultado es un DataFrame con las mismas columnas y datos que la lista original de diccionarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea472361",
   "metadata": {},
   "source": [
    "## Creación de DataFrame desde archivos\n",
    "\n",
    "Una de las formas más comunes de crear DataFrames en Spark es leer desde archivos. Spark soporta varios formatos de archivos como CSV, Parquet, Avro, entre otros. Aquí veremos cómo crear un DataFrame desde un archivo CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdef385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que tenemos un archivo 'data.csv' con las columnas 'name' y 'age'\n",
    "# df_from_csv = spark.read.csv('path_to_data.csv', header=True, inferSchema=True)\n",
    "# df_from_csv.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4a896",
   "metadata": {},
   "source": [
    "En el código anterior (comentado para evitar errores al no tener un archivo real), mostramos cómo leer un archivo CSV y convertirlo en un DataFrame. Utilizamos `spark.read.csv` y especificamos algunas opciones como `header=True` para indicar que la primera fila del CSV contiene los nombres de las columnas y `inferSchema=True` para que Spark infiera automáticamente el tipo de datos de cada columna. El resultado sería un DataFrame con los datos del archivo CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc71b8c",
   "metadata": {},
   "source": [
    "## SQL puro vs operaciones de DataFrame\n",
    "\n",
    "Spark SQL proporciona dos principales formas de interactuar con los datos:\n",
    "\n",
    "1. Usando SQL puro.\n",
    "2. Usando operaciones de DataFrame.\n",
    "\n",
    "Ambas formas tienen sus propias ventajas y depende del desarrollador decidir cuál usar según la situación. Veamos ejemplos de ambos para entender mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76465c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una vista temporal para ejecutar consultas SQL\n",
    "df_from_list_of_dicts.createOrReplaceTempView(\"people\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950d77d",
   "metadata": {},
   "source": [
    "Antes de poder ejecutar consultas SQL puro en un DataFrame, necesitamos crear una \"vista temporal\" de ese DataFrame. En el código anterior, hemos creado una vista temporal llamada \"people\" a partir de nuestro DataFrame `df_from_list_of_dicts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a4401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar una consulta SQL puro\n",
    "result_sql = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n",
    "result_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056136de",
   "metadata": {},
   "source": [
    "Usando la vista temporal \"people\", hemos ejecutado una consulta SQL puro para seleccionar nombres y edades de personas mayores de 30 años. El resultado es otro DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07bf392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar la misma consulta usando operaciones de DataFrame\n",
    "result_df = df_from_list_of_dicts.filter(df_from_list_of_dicts.age > 30).select(\"name\", \"age\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b18e8",
   "metadata": {},
   "source": [
    "Aquí, hemos logrado el mismo resultado que la consulta SQL puro, pero usando operaciones de DataFrame. Usamos el método `filter` para filtrar las filas y el método `select` para seleccionar las columnas deseadas. Ambos enfoques, ya sea SQL puro o operaciones de DataFrame, ofrecen flexibilidad y potencia en la manipulación y análisis de datos. La elección entre uno y otro a menudo se reduce a la familiaridad y preferencia personal del desarrollador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67001e1d",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (UDFs)\n",
    "\n",
    "En Spark, a veces puede que necesites realizar operaciones que no están directamente soportadas por las funciones incorporadas. En tales casos, puedes definir tus propias funciones, conocidas como UDFs (User Defined Functions). Estas UDFs se pueden usar en tus DataFrames para realizar transformaciones personalizadas en tus datos. Vamos a ver cómo crear y usar una UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672f48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+\n",
      "|age|     name|upper_name|\n",
      "+---+---------+----------+\n",
      "| 34|    Alice|     ALICE|\n",
      "| 45|      Bob|       BOB|\n",
      "| 29|Catherine| CATHERINE|\n",
      "+---+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Definir una UDF para convertir el nombre en mayúsculas\n",
    "def name_to_upper(name):\n",
    "    return name.upper()\n",
    "\n",
    "# Registrar la UDF\n",
    "name_to_upper_udf = udf(name_to_upper, StringType())\n",
    "\n",
    "# Aplicar la UDF al DataFrame\n",
    "df_with_upper_names = df_from_list_of_dicts.withColumn(\"upper_name\", name_to_upper_udf(df_from_list_of_dicts[\"name\"]))\n",
    "df_with_upper_names.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528aba56",
   "metadata": {},
   "source": [
    "En el código anterior, definimos una función `name_to_upper` que toma un nombre y devuelve su versión en mayúsculas. Luego, registramos esta función como una UDF llamada `name_to_upper_udf`. Después de registrar la UDF, podemos usarla como cualquier otra función en nuestro DataFrame.\n",
    "\n",
    "En el resultado, verás una nueva columna \"upper_name\" que contiene los nombres convertidos a mayúsculas. Las UDFs son extremadamente útiles para operaciones personalizadas que no están cubiertas por las funciones predeterminadas de Spark.\n",
    "\n",
    "Las UDFs proporcionan una gran flexibilidad al trabajar con DataFrames en Spark. Aunque Spark tiene una amplia gama de funciones incorporadas, las UDFs aseguran que siempre puedas realizar las transformaciones que necesitas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e3d35",
   "metadata": {},
   "source": [
    "## Ventanas y operaciones de ventana\n",
    "\n",
    "Las operaciones de ventana en Spark te permiten realizar cálculos en un conjunto definido de filas relacionadas con la fila actual dentro del DataFrame. Es similar a la función de GROUP BY, pero te permite mantener el resultado a nivel de fila en lugar de agruparlo. Estas operaciones son esenciales para ciertos cálculos, como los cálculos acumulativos.\n",
    "\n",
    "Vamos a ver un ejemplo donde calculamos el salario acumulado por país, ordenado por edad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19854b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|country|age|salary|\n",
      "+-------+---+------+\n",
      "|    USA| 25| 50000|\n",
      "|    USA| 30| 55000|\n",
      "|    USA| 35| 60000|\n",
      "| Mexico| 25| 40000|\n",
      "| Mexico| 30| 45000|\n",
      "| Mexico| 35| 50000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [\n",
    "    Row(country=\"USA\", age=25, salary=50000),\n",
    "    Row(country=\"USA\", age=30, salary=55000),\n",
    "    Row(country=\"USA\", age=35, salary=60000),\n",
    "    Row(country=\"Mexico\", age=25, salary=40000),\n",
    "    Row(country=\"Mexico\", age=30, salary=45000),\n",
    "    Row(country=\"Mexico\", age=35, salary=50000),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e241c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----------------+\n",
      "|country|age|salary|cumulative_salary|\n",
      "+-------+---+------+-----------------+\n",
      "| Mexico| 25| 40000|            40000|\n",
      "| Mexico| 30| 45000|            85000|\n",
      "| Mexico| 35| 50000|           135000|\n",
      "|    USA| 25| 50000|            50000|\n",
      "|    USA| 30| 55000|           105000|\n",
      "|    USA| 35| 60000|           165000|\n",
      "+-------+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Definir una ventana\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(\"age\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calcular el salario acumulado\n",
    "df_with_cumulative_salary = df.withColumn(\"cumulative_salary\", _sum(\"salary\").over(window_spec))\n",
    "df_with_cumulative_salary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc36811",
   "metadata": {},
   "source": [
    "En el código anterior, primero definimos una ventana con `Window.partitionBy(\"country\").orderBy(\"age\")`. Esto significa que queremos realizar cálculos dentro de cada país, ordenando las filas por edad. \n",
    "\n",
    "La parte `rowsBetween(Window.unboundedPreceding, Window.currentRow)` indica que para una fila dada, consideramos todas las filas anteriores (desde el principio de la ventana) hasta la fila actual para el cálculo.\n",
    "\n",
    "Luego, usamos la función `sum` con `over(window_spec)` para calcular el salario acumulado dentro de esa ventana.\n",
    "\n",
    "El resultado muestra el salario acumulado por país, con las filas ordenadas por edad dentro de cada país.\n",
    "\n",
    "Las operaciones de ventana son cruciales para realizar cálculos que requieren un contexto más amplio que la fila actual, especialmente en análisis financieros y de series temporales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801f47d",
   "metadata": {},
   "source": [
    "## Pivoteo de DataFrames\n",
    "\n",
    "El pivoteo es una técnica que permite transformar datos de un formato largo a un formato ancho. En otras palabras, es la conversión de datos que están en formato vertical (una columna) a un formato horizontal (muchas columnas). \n",
    "\n",
    "Spark permite pivotar fácilmente DataFrames usando el método `groupBy` junto con `pivot`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0105a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|country|ProductoA|ProductoB|\n",
      "+-------+---------+---------+\n",
      "|     MX|      100|      150|\n",
      "|     CA|       50|      300|\n",
      "|     US|      200|      250|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tenemos un DataFrame con ventas por producto y país\n",
    "data = [\n",
    "    (\"MX\", \"ProductoA\", 100),\n",
    "    (\"MX\", \"ProductoB\", 150),\n",
    "    (\"US\", \"ProductoA\", 200),\n",
    "    (\"US\", \"ProductoB\", 250),\n",
    "    (\"CA\", \"ProductoA\", 50),\n",
    "    (\"CA\", \"ProductoB\", 300)\n",
    "]\n",
    "df_sales = spark.createDataFrame(data, [\"country\", \"product\", \"sales\"])\n",
    "\n",
    "# Pivoteo del DataFrame para obtener ventas por producto en columnas separadas para cada país\n",
    "df_pivoted = df_sales.groupBy(\"country\").pivot(\"product\").sum(\"sales\")\n",
    "df_pivoted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e24aad",
   "metadata": {},
   "source": [
    "En el código anterior, creamos un DataFrame `df_sales` que contiene ventas por producto y país. A continuación, queremos pivotar este DataFrame para que cada producto tenga su propia columna y las filas representen a los países. \n",
    "\n",
    "El resultado es un DataFrame donde cada país tiene las ventas de \"ProductoA\" y \"ProductoB\" en columnas separadas.\n",
    "\n",
    "El pivoteo es especialmente útil cuando se tienen datos categóricos que se quieren convertir en múltiples columnas. Por ejemplo, si se tiene un registro de ventas por día y producto, y se quiere obtener un DataFrame donde cada columna represente un producto y las filas representen días, el pivoteo sería la técnica adecuada para realizar esta transformación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6746f8",
   "metadata": {},
   "source": [
    "## Manejo de fechas y timestamps\n",
    "\n",
    "Spark proporciona una serie de funciones para trabajar con fechas y timestamps. Estas funciones facilitan tareas como extraer componentes de una fecha (por ejemplo, el día, mes o año), calcular la diferencia entre fechas, agregar o restar días a una fecha, entre otras operaciones.\n",
    "\n",
    "El manejo de fechas y timestamps es crucial en muchos flujos de trabajo de procesamiento de datos, especialmente cuando se trata de series temporales, registros de logs, entre otros.\n",
    "\n",
    "Para ilustrar el uso de estas funciones, vamos a trabajar con un DataFrame que contiene timestamps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c8ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------------+\n",
      "|id |current_date|current_timestamp      |\n",
      "+---+------------+-----------------------+\n",
      "|0  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|1  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|2  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|3  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|4  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|5  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|6  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|7  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|8  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "|9  |2023-09-21  |2023-09-21 21:47:19.156|\n",
      "+---+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "# Creación de un DataFrame con fechas actuales y timestamps\n",
    "df_dates = spark.range(10).withColumn(\"current_date\", current_date()).withColumn(\"current_timestamp\", current_timestamp())\n",
    "df_dates.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27c9a2",
   "metadata": {},
   "source": [
    "En el código anterior, creamos un DataFrame `df_dates` que contiene una columna con la fecha actual y otra con el timestamp actual para cada fila. Hemos utilizado las funciones `current_date()` y `current_timestamp()` para obtener estos valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ede162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+---+----------------+\n",
      "|current_date|year|month|day|date_plus_5_days|\n",
      "+------------+----+-----+---+----------------+\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "|  2023-09-21|2023|    9| 21|      2023-09-26|\n",
      "+------------+----+-----+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, date_add\n",
    "\n",
    "# Extracción de componentes y suma de días a la fecha\n",
    "df_transformed = df_dates.select(\n",
    "    \"current_date\",\n",
    "    year(\"current_date\").alias(\"year\"),\n",
    "    month(\"current_date\").alias(\"month\"),\n",
    "    dayofmonth(\"current_date\").alias(\"day\"),\n",
    "    date_add(\"current_date\", 5).alias(\"date_plus_5_days\")\n",
    ")\n",
    "df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7920303",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos extraído el año, mes y día de la columna `current_date` usando las funciones `year()`, `month()` y `dayofmonth()`, respectivamente. También hemos añadido 5 días a la fecha original usando la función `date_add()`. \n",
    "\n",
    "Estas son solo algunas de las muchas operaciones que puedes realizar con fechas y timestamps en Spark. Las funciones relacionadas con fechas son esenciales para la manipulación y análisis de datos temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e3c7",
   "metadata": {},
   "source": [
    "## Manipulación de strings\n",
    "\n",
    "Spark SQL proporciona una amplia variedad de funciones para manipular strings en DataFrames. Estas funciones te permiten realizar tareas como cambiar a mayúsculas o minúsculas, substring, reemplazar caracteres, encontrar la longitud de un string, y muchas otras operaciones útiles.\n",
    "\n",
    "Vamos a explorar algunas de estas funciones con ejemplos prácticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117ff6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     full_name|\n",
      "+--------------+\n",
      "|      John Doe|\n",
      "|    Jane Smith|\n",
      "|Robert Johnson|\n",
      "|    Lucy Brown|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Creación de un DataFrame con una columna de strings\n",
    "data = [(\"John Doe\",), (\"Jane Smith\",), (\"Robert Johnson\",), (\"Lucy Brown\",)]\n",
    "df_strings = spark.createDataFrame(data, [\"full_name\"])\n",
    "df_strings.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdc75b",
   "metadata": {},
   "source": [
    "Para ilustrar la manipulación de strings, hemos creado un DataFrame `df_strings` que contiene una columna `full_name` con nombres completos de personas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46a41e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+-----------+-------------+\n",
      "|     full_name|    upper_name|    lower_name|name_length|first_4_chars|\n",
      "+--------------+--------------+--------------+-----------+-------------+\n",
      "|      John Doe|      JOHN DOE|      john doe|          8|         John|\n",
      "|    Jane Smith|    JANE SMITH|    jane smith|         10|         Jane|\n",
      "|Robert Johnson|ROBERT JOHNSON|robert johnson|         14|         Robe|\n",
      "|    Lucy Brown|    LUCY BROWN|    lucy brown|         10|         Lucy|\n",
      "+--------------+--------------+--------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper, lower, length, substring\n",
    "\n",
    "# Transformación y manipulación de strings\n",
    "df_transformed_strings = df_strings.select(\n",
    "    \"full_name\",\n",
    "    upper(\"full_name\").alias(\"upper_name\"),\n",
    "    lower(\"full_name\").alias(\"lower_name\"),\n",
    "    length(\"full_name\").alias(\"name_length\"),\n",
    "    substring(\"full_name\", 1, 4).alias(\"first_4_chars\")\n",
    ")\n",
    "df_transformed_strings.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e211fa0",
   "metadata": {},
   "source": [
    "En el código anterior, hemos aplicado varias transformaciones a la columna `full_name`:\n",
    "\n",
    "1. `upper_name`: Convertimos el nombre completo a mayúsculas.\n",
    "2. `lower_name`: Convertimos el nombre completo a minúsculas.\n",
    "3. `name_length`: Calculamos la longitud del nombre completo.\n",
    "4. `first_4_chars`: Extraemos los primeros 4 caracteres del nombre completo.\n",
    "\n",
    "Estas son solo algunas de las muchas funciones de manipulación de strings disponibles en Spark. Con estas funciones, puedes realizar una amplia variedad de operaciones en strings para adecuar, limpiar y transformar tus datos según tus necesidades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72409e",
   "metadata": {},
   "source": [
    "## Operaciones de Set (conjunto)\n",
    "\n",
    "Las operaciones de conjunto en Spark permiten trabajar con DataFrames como si fueran conjuntos, lo que facilita tareas como uniones, intersecciones y diferencias. Estas operaciones son esenciales cuando se necesita combinar o comparar datos de diferentes fuentes o DataFrames.\n",
    "\n",
    "Exploraremos algunas de las operaciones de conjunto más comunes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d0660b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     A|     1|\n",
      "|     B|     2|\n",
      "|     C|     3|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     B|     2|\n",
      "|     C|     3|\n",
      "|     D|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de dos DataFrames de ejemplo\n",
    "data1 = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n",
    "data2 = [(\"B\", 2), (\"C\", 3), (\"D\", 4)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"letter\", \"number\"])\n",
    "df2 = spark.createDataFrame(data2, [\"letter\", \"number\"])\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642c044",
   "metadata": {},
   "source": [
    "Hemos creado dos DataFrames, `df1` y `df2`, que contienen letras y números. Estos DataFrames se utilizarán para ilustrar las operaciones de conjunto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc22dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     A|     1|\n",
      "|     B|     2|\n",
      "|     C|     3|\n",
      "|     B|     2|\n",
      "|     C|     3|\n",
      "|     D|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unión de los DataFrames\n",
    "union_df = df1.union(df2)\n",
    "union_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723081d",
   "metadata": {},
   "source": [
    "La función `union` combina las filas de ambos DataFrames. Si hay filas duplicadas (como en nuestro caso con \"B\", 2 y \"C\", 3), estas aparecerán múltiples veces en el resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bc42efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "[Stage 58:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     C|     3|\n",
      "|     B|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Intersección de los DataFrames\n",
    "intersect_df = df1.intersect(df2)\n",
    "intersect_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f8ce8",
   "metadata": {},
   "source": [
    "La función `intersect` devuelve solo las filas que están presentes en ambos DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a007d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     A|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diferencia de los DataFrames\n",
    "diff_df = df1.subtract(df2)\n",
    "diff_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7c6f8",
   "metadata": {},
   "source": [
    "La función `subtract` devuelve las filas que están en `df1` pero no en `df2`. Es decir, muestra la diferencia entre los dos DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112f7ac",
   "metadata": {},
   "source": [
    "## Lectura y Escritura en Diferentes Formatos\n",
    "\n",
    "Apache Spark es conocido por su capacidad de trabajar con una amplia variedad de fuentes de datos. Puede leer y escribir en formatos populares como CSV, Parquet, JSON, entre otros. Esta flexibilidad permite a Spark integrarse fácilmente en arquitecturas de datos y trabajar con diferentes sistemas de almacenamiento.\n",
    "\n",
    "A continuación, exploraremos cómo leer y escribir en algunos de estos formatos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "218bdced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de un archivo CSV\n",
    "#csv_path = \"/path/to/your/csv/file.csv\"\n",
    "#csv_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "#csv_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cab9b",
   "metadata": {},
   "source": [
    "Para leer archivos CSV, utilizamos la función `read.csv` de Spark. La opción `header=True` indica que la primera fila del archivo contiene los nombres de las columnas. Con `inferSchema=True`, Spark intenta inferir automáticamente el tipo de datos de cada columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b77bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura en formato Parquet\n",
    "#parquet_path = \"/path/to/save/parquet/file.parquet\"\n",
    "#csv_df.write.parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4796d",
   "metadata": {},
   "source": [
    "Parquet es un formato de archivo columnar que es altamente eficiente para operaciones analíticas. Spark puede escribir DataFrames directamente en formato Parquet usando la función `write.parquet`. Este formato es especialmente útil cuando trabajamos con grandes conjuntos de datos, ya que ofrece una compresión eficiente y mejora el rendimiento de las consultas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "546b2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de un archivo JSON\n",
    "#json_path = \"/path/to/your/json/file.json\"\n",
    "#json_df = spark.read.json(json_path)\n",
    "#json_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c712cd",
   "metadata": {},
   "source": [
    "El formato JSON es ampliamente utilizado para la representación de datos estructurados. Spark proporciona la función `read.json` para leer archivos en este formato. A diferencia de otros formatos, JSON no requiere que se defina un esquema por adelantado; Spark puede inferir el esquema directamente del archivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adf8bf",
   "metadata": {},
   "source": [
    "## Tratamiento de Valores Nulos\n",
    "\n",
    "En la vida real, los conjuntos de datos suelen tener valores faltantes o nulos debido a diversas razones. Estos valores nulos pueden afectar el resultado de cualquier operación o análisis. Por lo tanto, es crucial tratar adecuadamente estos valores antes de realizar cualquier procesamiento o análisis.\n",
    "\n",
    "Spark ofrece varias herramientas para identificar, eliminar o reemplazar estos valores nulos. Vamos a explorar algunas de estas herramientas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aab16a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, col\n",
    "\n",
    "# Supongamos que tenemos el siguiente DataFrame\n",
    "data_with_nulls = [\n",
    "    (\"Alice\", None, 25),\n",
    "    (\"Bob\", \"Engineering\", None),\n",
    "    (\"Charlie\", \"Finance\", 30),\n",
    "    (\"David\", None, 28)\n",
    "]\n",
    "\n",
    "df_with_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"department\", \"age\"])\n",
    "\n",
    "# Identificar valores nulos\n",
    "null_counts = df_with_nulls.select([isnull(col(c)).alias(c) for c in df_with_nulls.columns]).groupby().sum().collect()[0]\n",
    "null_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3d787",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, creamos un DataFrame `df_with_nulls` con algunos valores nulos. Utilizamos la función `isnull` junto con un `select` para identificar qué valores en el DataFrame son nulos. Luego, con la función `groupby().sum()`, sumamos el total de valores nulos por columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "341b373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+\n",
      "|   name|department|age|\n",
      "+-------+----------+---+\n",
      "|Charlie|   Finance| 30|\n",
      "+-------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con valores nulos\n",
    "df_no_nulls = df_with_nulls.na.drop()\n",
    "df_no_nulls.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6ceb6",
   "metadata": {},
   "source": [
    "Para eliminar las filas que contienen valores nulos, podemos usar el método `na.drop()` en un DataFrame. Sin embargo, esta es una operación drástica, ya que podría resultar en la pérdida de datos valiosos. Es recomendable usarla con precaución y asegurarse de que es la mejor opción para el contexto específico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44ae2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+\n",
      "|   name| department|age|\n",
      "+-------+-----------+---+\n",
      "|  Alice|    Unknown| 25|\n",
      "|    Bob|Engineering| -1|\n",
      "|Charlie|    Finance| 30|\n",
      "|  David|    Unknown| 28|\n",
      "+-------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar valores nulos con valores por defecto\n",
    "values_to_replace = {\"department\": \"Unknown\", \"age\": -1}\n",
    "df_replaced = df_with_nulls.na.fill(values_to_replace)\n",
    "df_replaced.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728b80b",
   "metadata": {},
   "source": [
    "Otra opción es reemplazar los valores nulos con valores predeterminados. En el ejemplo anterior, reemplazamos los valores nulos en la columna `department` con la cadena \"Unknown\" y los valores nulos en la columna `age` con -1. La función `na.fill()` es útil para esta tarea y toma un diccionario que especifica los valores de reemplazo por columna.\n",
    "\n",
    "El manejo adecuado de valores nulos es crucial para garantizar la calidad y precisión de los resultados. Es importante decidir cuidadosamente cómo se tratarán estos valores, teniendo en cuenta el contexto y las implicaciones de cada elección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c72ecc",
   "metadata": {},
   "source": [
    "## Reparticionamiento y Coalescencia\n",
    "\n",
    "Los DataFrames en Spark están divididos en particiones que representan una fracción del conjunto total de datos. El número de particiones determina cómo se distribuyen los datos en los nodos del clúster y afecta directamente el rendimiento de las operaciones. Es posible que, en ciertos escenarios, necesitemos ajustar el número de particiones para optimizar el rendimiento.\n",
    "\n",
    "Spark proporciona dos operaciones principales para este propósito: `repartition` y `coalesce`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1288532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando un DataFrame\n",
    "data = [(i, f\"val_{i}\") for i in range(100)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "# Observando el número de particiones\n",
    "num_partitions = df.rdd.getNumPartitions()\n",
    "num_partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005383e",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, hemos creado un DataFrame `df` y hemos verificado el número de particiones usando `getNumPartitions()`. El número de particiones por defecto puede variar según la configuración de Spark y el tamaño del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79b9e063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiando el número de particiones\n",
    "df_repartitioned = df.repartition(5)\n",
    "\n",
    "# Verificando el nuevo número de particiones\n",
    "new_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "new_partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7f607",
   "metadata": {},
   "source": [
    "La función `repartition` permite cambiar el número de particiones. En este caso, hemos reparticionado el DataFrame `df` para que tenga 5 particiones. Es importante notar que la operación de reparticionamiento puede ser costosa en términos de tiempo y recursos, ya que implica el movimiento de datos entre las particiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38eaf0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduciendo el número de particiones\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "\n",
    "# Verificando el nuevo número de particiones\n",
    "coalesced_partitions = df_coalesced.rdd.getNumPartitions()\n",
    "coalesced_partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf222a",
   "metadata": {},
   "source": [
    "La operación `coalesce` es similar a `repartition`, pero está diseñada específicamente para reducir el número de particiones. La ventaja de `coalesce` sobre `repartition` es que no implica el movimiento completo de datos entre las particiones y, por lo tanto, es más eficiente cuando se necesita reducir el número de particiones.\n",
    "\n",
    "Es vital comprender el impacto del número de particiones en el rendimiento y saber cuándo y cómo ajustar este número. Las operaciones de reparticionamiento y coalescencia son herramientas valiosas en el arsenal de un ingeniero de datos para optimizar las operaciones en Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dba8f4",
   "metadata": {},
   "source": [
    "## Estadísticas Descriptivas\n",
    "\n",
    "Spark ofrece una serie de funciones integradas que facilitan la obtención de estadísticas descriptivas de un DataFrame. Estas estadísticas son esenciales para comprender las tendencias, variaciones y distribución de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7a60ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|value|\n",
      "+--------+-----+\n",
      "|       A|    1|\n",
      "|       B|    2|\n",
      "|       A|    3|\n",
      "|       B|    4|\n",
      "|       A|    5|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Creando un DataFrame de ejemplo\n",
    "data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"B\", 4), (\"A\", 5)]\n",
    "df_stats = spark.createDataFrame(data, [\"category\", \"value\"])\n",
    "\n",
    "df_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f93e708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 92:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|summary|category|             value|\n",
      "+-------+--------+------------------+\n",
      "|  count|       5|                 5|\n",
      "|   mean|    null|               3.0|\n",
      "| stddev|    null|1.5811388300841898|\n",
      "|    min|       A|                 1|\n",
      "|    max|       B|                 5|\n",
      "+-------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculando estadísticas descriptivas básicas\n",
    "df_stats.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3f9e3",
   "metadata": {},
   "source": [
    "La función `describe` proporciona estadísticas descriptivas básicas como el recuento, la media, la desviación estándar, el mínimo y el máximo para todas las columnas numéricas del DataFrame. Es una herramienta útil para obtener un resumen rápido de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d90d0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------------+---+---+\n",
      "|category|mean|            stddev|min|max|\n",
      "+--------+----+------------------+---+---+\n",
      "|       A| 3.0|               2.0|  1|  5|\n",
      "|       B| 3.0|1.4142135623730951|  2|  4|\n",
      "+--------+----+------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculando estadísticas agrupadas por 'category'\n",
    "df_stats.groupBy(\"category\").agg(\n",
    "    F.mean(\"value\").alias(\"mean\"),\n",
    "    F.stddev(\"value\").alias(\"stddev\"),\n",
    "    F.min(\"value\").alias(\"min\"),\n",
    "    F.max(\"value\").alias(\"max\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe69ec2",
   "metadata": {},
   "source": [
    "Podemos combinar las operaciones de agrupación (`groupBy`) con funciones de agregación para calcular estadísticas descriptivas para diferentes categorías o grupos en el DataFrame. En el ejemplo anterior, calculamos la media, desviación estándar, mínimo y máximo para cada `category` en el DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025bf25",
   "metadata": {},
   "source": [
    "## Correlación y Covarianza\n",
    "\n",
    "La correlación y la covarianza son dos medidas que indican la relación entre dos variables:\n",
    "\n",
    "1. **Correlación:** Mide el grado en el que dos variables cambian juntas. Si una variable tiende a aumentar cuando la otra aumenta, hay una correlación positiva. Si una variable tiende a disminuir cuando la otra aumenta, hay una correlación negativa. \n",
    "2. **Covarianza:** Es similar a la correlación, pero no está normalizada. Así, mientras que la correlación está limitada entre -1 y 1, la covarianza puede ser cualquier número.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef94043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación: 1.0\n",
      "Covarianza: 2.5\n"
     ]
    }
   ],
   "source": [
    "# Calculando la correlación entre las columnas\n",
    "correlation = df_stats.corr(\"value\", \"value\")\n",
    "print(f\"Correlación: {correlation}\")\n",
    "\n",
    "# Calculando la covarianza entre las columnas\n",
    "covariance = df_stats.cov(\"value\", \"value\")\n",
    "print(f\"Covarianza: {covariance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e164c",
   "metadata": {},
   "source": [
    "## Frecuencia\n",
    "\n",
    "La frecuencia de un valor es el número de veces que aparece en el conjunto de datos. Podemos utilizar la operación `groupBy` junto con `count` para calcular las frecuencias de los diferentes valores en una columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a28550f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|       A|    3|\n",
      "|       B|    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculando la frecuencia de cada 'category'\n",
    "frequency = df_stats.groupBy(\"category\").count()\n",
    "frequency.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7b592",
   "metadata": {},
   "source": [
    "La tabla anterior muestra la cantidad de veces que cada categoría aparece en el DataFrame. Esta información puede ser útil para comprender la distribución de los datos en diferentes categorías.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae2789",
   "metadata": {},
   "source": [
    "## Sampleo y Partición de DataFrames\n",
    "\n",
    "El sampleo se refiere al proceso de seleccionar una muestra aleatoria de nuestros datos. Esto es útil para trabajar con un subconjunto representativo de nuestros datos, especialmente cuando el DataFrame es muy grande.\n",
    "\n",
    "La partición, por otro lado, implica dividir el DataFrame en varios subconjuntos. Un uso común es en machine learning, donde se divide el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba.\n",
    "\n",
    "El sampleo y la partición son herramientas poderosas en el arsenal del ingeniero de datos y del científico de datos. Permiten trabajar con conjuntos de datos manejables y preparar datos para diferentes etapas del análisis y modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ac582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38e855c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  3| val_3|\n",
      "| 21|val_21|\n",
      "| 24|val_24|\n",
      "| 27|val_27|\n",
      "| 28|val_28|\n",
      "| 34|val_34|\n",
      "| 41|val_41|\n",
      "| 44|val_44|\n",
      "| 46|val_46|\n",
      "| 50|val_50|\n",
      "| 53|val_53|\n",
      "| 64|val_64|\n",
      "| 74|val_74|\n",
      "| 80|val_80|\n",
      "| 84|val_84|\n",
      "| 86|val_86|\n",
      "| 91|val_91|\n",
      "| 94|val_94|\n",
      "| 97|val_97|\n",
      "| 98|val_98|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tomar un sampleo del 20% de los datos sin reemplazo\n",
    "sampled_df = df.sample(False, 0.2)\n",
    "sampled_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d90589",
   "metadata": {},
   "source": [
    "En el código anterior, hemos tomado un sampleo del 20% de nuestro DataFrame original (`df`). El primer argumento (`False`) indica que no queremos reemplazo, lo que significa que una vez que se selecciona una fila, no se puede seleccionar de nuevo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "443743ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el DataFrame en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae33577",
   "metadata": {},
   "source": [
    "El método `randomSplit` permite dividir el DataFrame en múltiples subconjuntos según las proporciones dadas. En el ejemplo anterior, dividimos el DataFrame en un conjunto de entrenamiento del 80% y un conjunto de prueba del 20%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72868eb9",
   "metadata": {},
   "source": [
    "## Caching y Persistencia\n",
    "\n",
    "El caching y la persistencia son técnicas que permiten almacenar DataFrames (o RDDs) en la memoria o en el disco para que las operaciones repetitivas sobre esos DataFrames sean más rápidas. Esto es especialmente útil cuando se tienen operaciones de transformación costosas que se quieren evitar repetir.\n",
    "\n",
    "La diferencia principal entre caching y persistencia radica en la configuración. Mientras que el caching almacena el DataFrame en la memoria por defecto, la persistencia permite elegir dónde se quiere almacenar el DataFrame (memoria, disco, o ambos).\n",
    "\n",
    "El caching y la persistencia son esenciales cuando se trabaja con operaciones iterativas o se desea optimizar el rendimiento de operaciones repetitivas sobre un DataFrame. Es importante considerar el equilibrio entre el uso de memoria y el rendimiento para decidir cuál técnica utilizar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16df0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  0| val_0|\n",
      "|  1| val_1|\n",
      "|  2| val_2|\n",
      "|  3| val_3|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  6| val_6|\n",
      "|  7| val_7|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 13|val_13|\n",
      "| 14|val_14|\n",
      "| 15|val_15|\n",
      "| 16|val_16|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Realizar operaciones en el DataFrame cacheado\n",
    "df.count()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c47d3e",
   "metadata": {},
   "source": [
    "En el código anterior, hemos cacheado el DataFrame `df` en la memoria. Una vez que un DataFrame está cacheado, cualquier operación sobre ese DataFrame será más rápida después de la primera vez, ya que los datos ya están en la memoria y Spark no necesita recomputar nada desde el origen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0e37ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/21 22:54:51 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  0| val_0|\n",
      "|  1| val_1|\n",
      "|  2| val_2|\n",
      "|  3| val_3|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  6| val_6|\n",
      "|  7| val_7|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 13|val_13|\n",
      "| 14|val_14|\n",
      "| 15|val_15|\n",
      "| 16|val_16|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, value: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Persistir el DataFrame en memoria y disco\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Realizar operaciones en el DataFrame persistido\n",
    "df.count()\n",
    "df.show()\n",
    "\n",
    "# Eliminar la persistencia\n",
    "df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da7f13",
   "metadata": {},
   "source": [
    "La persistencia permite más flexibilidad que el simple caching. En el ejemplo anterior, hemos persistido el DataFrame `df` tanto en la memoria como en el disco. Si la memoria se llena, Spark comenzará a escribir los datos en el disco. Una vez persistido, al igual que con el caching, las operaciones subsiguientes sobre ese DataFrame se beneficiarán de tiempos de acceso más rápidos.\n",
    "\n",
    "Es importante recordar liberar recursos después de usarlos, por lo que es una buena práctica utilizar `unpersist()` para eliminar un DataFrame de la caché o persistencia cuando ya no es necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee477f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
