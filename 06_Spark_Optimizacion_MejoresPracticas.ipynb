{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bd5b02",
   "metadata": {},
   "source": [
    "## Técnicas de optimización en Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5663668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 20:10:24 WARN Utils: Your hostname, MacBook-Air-de-Ivan.local resolves to a loopback address: 127.0.0.1; using 192.168.0.2 instead (on interface en0)\n",
      "23/09/25 20:10:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 20:10:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/25 20:10:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Optimizacion\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a750420",
   "metadata": {},
   "source": [
    "Apache Spark es conocido por ser un sistema de computación en clúster rápido y general. Aunque es eficiente por diseño, hay varias técnicas que los desarrolladores pueden emplear para maximizar el rendimiento y aprovechar al máximo Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04958a70",
   "metadata": {},
   "source": [
    "La optimización en Spark se centra en mejorar la eficiencia y la velocidad de procesamiento de las operaciones, minimizando los recursos utilizados y el tiempo de espera. Esto es crucial cuando se manejan grandes conjuntos de datos y se realizan operaciones intensivas. A continuación, discutiremos algunas de las técnicas comunes de optimización en Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822e298",
   "metadata": {},
   "source": [
    "### Uso de Broadcast Variables\n",
    "\n",
    "Las variables de transmisión permiten mantener una variable de solo lectura en caché en cada máquina en lugar de enviar una copia de ella con las tareas. Son útiles cuando las tareas a través de múltiples etapas necesitan la misma variable de solo lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162b7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 21:49:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|value1|value2|\n",
      "+---+------+------+\n",
      "|  A|    10|     1|\n",
      "|  B|    20|     2|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Optimization Techniques\").getOrCreate()\n",
    "\n",
    "# Creando DataFrames de ejemplo\n",
    "data_large = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40)]\n",
    "data_small = [(\"A\", 1), (\"B\", 2)]\n",
    "df_large = spark.createDataFrame(data_large, [\"id\", \"value1\"])\n",
    "df_small = spark.createDataFrame(data_small, [\"id\", \"value2\"])\n",
    "\n",
    "# Usando broadcast para el DataFrame más pequeño durante la operación join\n",
    "joined_df = df_large.join(broadcast(df_small), \"id\")\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d783c6",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, primero creamos dos DataFrames: uno grande (`df_large`) y uno pequeño (`df_small`). Al hacer un `join` entre ellos, utilizamos la función `broadcast` en el DataFrame más pequeño. Esto asegura que el DataFrame pequeño se envíe a todos los nodos solo una vez, permitiendo que Spark realice el join de manera más eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f006f",
   "metadata": {},
   "source": [
    "### Uso de Data Partitioning\n",
    "\n",
    "El particionado es una técnica para distribuir equitativamente los datos a través de las particiones para optimizar la paralelización y minimizar la cantidad de datos que se envían a través de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfd0366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de particiones antes del reparticionado: 2\n",
      "Número de particiones después del reparticionado: 4\n"
     ]
    }
   ],
   "source": [
    "# Creando un RDD de ejemplo\n",
    "rdd = spark.sparkContext.parallelize([(1, \"A\", 100), (2, \"B\", 200), (3, \"C\", 300), (4, \"D\", 400)], 2)\n",
    "\n",
    "# Transformando el RDD a DataFrame\n",
    "df = spark.createDataFrame(rdd, [\"id\", \"name\", \"value\"])\n",
    "\n",
    "# Mostrar el número de particiones antes del reparticionado\n",
    "print(\"Número de particiones antes del reparticionado:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Reparticionando el DataFrame en 4 particiones\n",
    "df_repartitioned = df.repartition(4)\n",
    "\n",
    "# Mostrar el número de particiones después del reparticionado\n",
    "print(\"Número de particiones después del reparticionado:\", df_repartitioned.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c3fd7",
   "metadata": {},
   "source": [
    "En este ejemplo, primero creamos un RDD llamado `rdd` con 2 particiones. Después, convertimos ese RDD en un DataFrame (`df`). A continuación, mostramos el número de particiones antes y después de reparticionar el DataFrame. Usamos el método `repartition(4)` para cambiar el número de particiones a 4. El reparticionado puede ser útil para distribuir los datos de manera uniforme y optimizar las operaciones paralelas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff64c10",
   "metadata": {},
   "source": [
    "### Configuración y ajuste del rendimiento\n",
    "\n",
    "Spark ofrece múltiples configuraciones que pueden ser ajustadas para optimizar el rendimiento según las características del trabajo y la arquitectura del clúster.\n",
    "\n",
    "### Configuración de Memoria\n",
    "\n",
    "La memoria en Spark se divide en dos regiones: memoria de ejecución y memoria de almacenamiento. La configuración adecuada de la memoria puede mejorar significativamente el rendimiento de las aplicaciones Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995bac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per node: Not Defined\n",
      "Max result size: Not Defined\n",
      "Memory fraction: Not Defined\n",
      "Storage memory fraction: Not Defined\n"
     ]
    }
   ],
   "source": [
    "def get_spark_conf(key, default=\"Not Defined\"):\n",
    "    try:\n",
    "        return spark.conf.get(key)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# Mostrar la configuración actual de memoria\n",
    "spark_memory_conf = {\n",
    "    \"Memory per node\": get_spark_conf(\"spark.driver.memory\"),\n",
    "    \"Max result size\": get_spark_conf(\"spark.driver.maxResultSize\"),\n",
    "    \"Memory fraction\": get_spark_conf(\"spark.memory.fraction\"),\n",
    "    \"Storage memory fraction\": get_spark_conf(\"spark.memory.storageFraction\")\n",
    "}\n",
    "\n",
    "for key, value in spark_memory_conf.items():\n",
    "    print(key + \": \" + value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3647f8",
   "metadata": {},
   "source": [
    "\n",
    "La configuración de memoria en Spark es esencial para garantizar que tus trabajos se ejecuten de manera eficiente y sin errores relacionados con la memoria. Aunque no todas las configuraciones están explícitamente definidas en todos los entornos de Spark, es importante conocerlas y, si es necesario, ajustarlas según las necesidades de tu trabajo y los recursos disponibles.\n",
    "\n",
    "Aquí están las configuraciones de memoria que intentamos obtener, junto con sus valores predeterminados en Spark (estos valores pueden variar según la versión de Spark y la configuración del clúster):\n",
    "\n",
    "- **Memory per node (spark.driver.memory)**: Memoria asignada por nodo. Valor predeterminado: 1g.\n",
    "- **Max result size (spark.driver.maxResultSize)**: Tamaño máximo de resultados que se pueden recopilar en el controlador. Valor predeterminado: 1g.\n",
    "- **Memory fraction (spark.memory.fraction)**: Fracción del heap de Java destinado a Spark. Valor predeterminado: 0.6.\n",
    "- **Storage memory fraction (spark.memory.storageFraction)**: Fracción del heap de memoria de Spark destinado al almacenamiento. Valor predeterminado: 0.5.\n",
    "\n",
    "Para obtener más detalles sobre estas configuraciones y otros ajustes relacionados con la memoria, consulta la [documentación oficial de Spark](https://spark.apache.org/docs/latest/configuration.html#application-properties).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75585d",
   "metadata": {},
   "source": [
    "### Optimización de la Serialización\n",
    "\n",
    "La serialización es el proceso de convertir un objeto en un flujo de bytes para su almacenamiento o transmisión. \n",
    "\n",
    "Spark utiliza la serialización para:\n",
    "\n",
    "Transmitir datos a través de la red durante operaciones de shuffle.\n",
    "\n",
    "Almacenar datos en memoria.\n",
    "\n",
    "Spark proporciona dos bibliotecas de serialización:\n",
    "\n",
    "Java (predeterminada): Proporciona un buen equilibrio entre velocidad y serialización de cualquier objeto. Sin embargo, los datos serializados suelen ser más grandes, lo que puede aumentar el tiempo de transmisión y el uso de la memoria.\n",
    "\n",
    "Kryo: Es más rápido y produce datos serializados más pequeños, pero no admite todos los tipos de objetos por defecto.\n",
    "\n",
    "Para configurar Spark para usar Kryo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5f157",
   "metadata": {},
   "source": [
    "### Evitar Operaciones Costosas\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Supongamos que tenemos el siguiente RDD y queremos sumar valores por clave:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b680a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, 2), (3, 4), (3, 6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d81b9",
   "metadata": {},
   "source": [
    "En lugar de usar `groupByKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a03ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = rdd.groupByKey().mapValues(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d46520",
   "metadata": {},
   "source": [
    "Es más eficiente usar `reduceByKey`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "966abd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = rdd.reduceByKey(lambda x, y: x + y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c62b2",
   "metadata": {},
   "source": [
    "### Caché y Persistencia\n",
    "\n",
    "Si vamos a utilizar un DataFrame o RDD múltiples veces, podemos beneficiarnos del uso de `cache()` o `persist()`.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Supongamos que tenemos un DataFrame y queremos realizar varias operaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e0249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(id=i, value=i**2) for i in range(100)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd0a56",
   "metadata": {},
   "source": [
    "Podemos cachearlo para acelerar operaciones posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c400eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 22:11:23 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  0|    0|\n",
      "|  1|    1|\n",
      "|  2|    4|\n",
      "|  3|    9|\n",
      "|  4|   16|\n",
      "|  5|   25|\n",
      "|  6|   36|\n",
      "|  7|   49|\n",
      "|  8|   64|\n",
      "|  9|   81|\n",
      "| 10|  100|\n",
      "| 11|  121|\n",
      "| 12|  144|\n",
      "| 13|  169|\n",
      "| 14|  196|\n",
      "| 15|  225|\n",
      "| 16|  256|\n",
      "| 17|  289|\n",
      "| 18|  324|\n",
      "| 19|  361|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  0|    1|\n",
      "|  7|    1|\n",
      "|  6|    1|\n",
      "|  9|    1|\n",
      "|  5|    1|\n",
      "|  1|    1|\n",
      "| 10|    1|\n",
      "|  3|    1|\n",
      "|  8|    1|\n",
      "| 11|    1|\n",
      "|  2|    1|\n",
      "|  4|    1|\n",
      "| 19|    1|\n",
      "| 22|    1|\n",
      "| 17|    1|\n",
      "| 12|    1|\n",
      "| 13|    1|\n",
      "| 18|    1|\n",
      "| 14|    1|\n",
      "| 21|    1|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "\n",
    "# Operaciones posteriores se benefician del caché\n",
    "df.filter(\"id < 50\").show()\n",
    "\n",
    "\n",
    "df.groupBy(\"id\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1d367",
   "metadata": {},
   "source": [
    "Cuando aplicamos transformaciones y acciones a df, como el filtro, la muestra y el groupBy, si no hubiéramos utilizado df.cache(), Spark podría haber recalculado df desde el principio cada vez. Esto podría haber sido especialmente costoso si df se hubiera derivado de una serie de transformaciones complejas.\n",
    "\n",
    "Sin embargo, dado que usamos df.cache(), después de que df se calculó y se cargó en la memoria la primera vez, todas las acciones subsiguientes (filtrado, muestra, etc.) se realizaron mucho más rápido, ya que utilizaron la versión en caché de df.\n",
    "\n",
    "Cada vez que visualizamos o interactuamos con el DataFrame df, estamos aprovechando el beneficio del caché. Si df fuera grande y derivado de múltiples operaciones, el beneficio de usar caché sería aún más notorio en términos de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ca752",
   "metadata": {},
   "source": [
    "### Monitorizar y Utilizar la UI de Spark\n",
    "\n",
    "La interfaz de usuario de Spark (Spark UI) es una herramienta esencial para la monitorización y depuración de aplicaciones Spark. Proporciona una visión detallada de la ejecución de trabajos, etapas y tareas, así como métricas de rendimiento y detalles sobre el uso de recursos.\n",
    "\n",
    "#### Acceder a la Spark UI\n",
    "\n",
    "Por defecto, Spark UI está habilitada y se puede acceder a ella a través de un navegador web en el puerto 4040 de la máquina donde se ejecuta el controlador de Spark. La URL suele ser http://[driver-node]:4040.\n",
    "\n",
    "#### Componentes principales de Spark UI:\n",
    "1. Página de resumen (Jobs)\n",
    "Aquí se muestra una lista de trabajos ejecutados, en ejecución y pendientes. Puedes ver detalles como el ID del trabajo, la duración, las etapas y las tareas asociadas, así como el estado del trabajo (completado, en ejecución, fallido).\n",
    "\n",
    "2. Stages\n",
    "Esta pestaña muestra detalles sobre las etapas de los trabajos. Una etapa es una unidad de trabajo en Spark, y cada trabajo puede tener múltiples etapas. Puedes ver detalles como el número de tareas, la duración, los datos leídos, y los bytes y registros procesados.\n",
    "\n",
    "3. Storage\n",
    "Muestra información sobre los RDDs y DataFrames almacenados en memoria o en disco. Es útil para monitorizar el uso de la memoria y el almacenamiento en caché.\n",
    "\n",
    "4. Environment\n",
    "Proporciona información sobre la configuración de Spark y los detalles del entorno. Es útil para verificar y depurar la configuración de Spark.\n",
    "\n",
    "5. Executors\n",
    "Muestra detalles sobre los ejecutores, que son procesos en nodos de trabajador que ejecutan tareas. Puedes ver métricas como la memoria utilizada, la CPU y el número de tareas completadas.\n",
    "\n",
    "6. SQL\n",
    "Si estás ejecutando consultas Spark SQL, esta pestaña muestra detalles sobre la ejecución de esas consultas, incluidos los planes de ejecución físicos y lógicos.\n",
    "\n",
    "#### Ejemplo práctico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tenemos el siguiente código que lee un archivo CSV y realiza algunas transformaciones.\n",
    "data = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n",
    "data.groupBy(\"some_column\").count().show()\n",
    "\n",
    "# Después de ejecutar este código, puedes ir a Spark UI y:\n",
    "# 1. Navega a la pestaña \"Jobs\" para ver el trabajo ejecutado.\n",
    "# 2. Haz clic en el ID del trabajo para ver detalles sobre las etapas y tareas.\n",
    "# 3. Navega a la pestaña \"Stages\" para ver más detalles sobre las etapas.\n",
    "# 4. Si almacenaste algo en caché, verifica la pestaña \"Storage\" para ver su tamaño y ubicación (memoria o disco).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b258b1",
   "metadata": {},
   "source": [
    "#### Recomendaciones para usar Spark UI:\n",
    "\n",
    "Depuración de errores: Si un trabajo o etapa falla, Spark UI es el primer lugar donde debes buscar detalles sobre el error.\n",
    "\n",
    "Optimización: Si notas que un trabajo está llevando más tiempo del esperado, verifica las etapas y tareas para identificar cuellos de botella.\n",
    "\n",
    "Monitorización del uso de recursos: Verifica el uso de memoria y CPU en la pestaña \"Executors\" para asegurarte de que tu aplicación está utilizando los recursos de manera eficiente.\n",
    "\n",
    "La UI de Spark es una herramienta esencial para cualquier desarrollador o administrador de Spark. Proporciona una gran cantidad de información que puede ayudarte a optimizar y depurar tus aplicaciones Spark. Es una buena práctica familiarizarse con ella y consultarla regularmente mientras desarrollas y ejecutas aplicaciones Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea62b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
