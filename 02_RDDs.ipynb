{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750fc247-9892-46c8-83ea-04e78bc284ee",
   "metadata": {},
   "source": [
    "### Configuración Inicial\n",
    "\n",
    "Para comenzar con la utilización de Spark y RDDs, es necesario realizar una configuración inicial:\n",
    "\n",
    "1. **`findspark.init()`:** Este comando localiza e inicializa una instalación de Spark en tu máquina, permitiendo que Spark se integre con Jupyter.\n",
    "2. **`SparkSession.builder...`:** Aquí estamos creando una instancia de SparkSession, que es el punto de entrada para cualquier funcionalidad en Spark. `master(\"local[*]\")` le dice a Spark que corra localmente en tu máquina utilizando todos los núcleos disponibles. `appName(\"RDDPractice\")` simplemente le da un nombre a tu SparkSession para identificación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821aa2a3-c42e-4406-baee-c0aad31f97f9",
   "metadata": {},
   "source": [
    "### Spark Core y RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae9eba9-dee5-4166-b227-624fcaa3ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/21 16:28:19 WARN Utils: Your hostname, MacBook-Air-de-Ivan.local resolves to a loopback address: 127.0.0.1; using 192.168.0.2 instead (on interface en0)\n",
      "23/09/21 16:28:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/21 16:28:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/21 16:28:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/21 16:28:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/21 16:28:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "# Importar findspark y inicializarlo\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Importar SparkSession y crear una instancia\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"RDDPractice\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720483e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "373a3f24-406c-481c-b223-4caf9fdf6a17",
   "metadata": {},
   "source": [
    "### Creación de RDD\n",
    "\n",
    "En esta sección, creamos un RDD a partir de una lista de nombres. Aquí están los pasos clave:\n",
    "\n",
    "1. **`spark.sparkContext.parallelize(data)`:** Usamos el método `parallelize` para convertir una lista en un RDD. Este método distribuye los elementos de la lista a través de las diferentes particiones para procesamiento paralelo.\n",
    "2. **`rdd_from_list.collect()`:** El método `collect` se utiliza para recuperar todos los elementos del RDD y mostrarlos. Es importante usar `collect` con precaución, ya que recopila todos los datos en la memoria del driver, lo que puede causar problemas si el RDD es muy grande.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e0e3dc-de89-4b61-8fff-4f33befefbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Alice', 'Bob', 'Charlie', 'David', 'Eva']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando un RDD desde una lista\n",
    "data = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"]\n",
    "rdd_from_list = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Mostrando el contenido del RDD\n",
    "rdd_from_list.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdc3f58-df0a-47a8-b519-e7a10062c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.2:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDDPractice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114463310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add43df8-9048-4d48-8d51-54e60dc29736",
   "metadata": {},
   "source": [
    "### Transformaciones en RDD\n",
    "\n",
    "Las transformaciones permiten modificar los datos en el RDD sin alterar el RDD original. En este segmento, trabajamos con dos transformaciones comunes: `map` y `filter`.\n",
    "\n",
    "1. **`map`:** Esta transformación aplica una función a cada elemento del RDD. En nuestro caso, hemos convertido cada nombre a mayúsculas utilizando una función lambda.\n",
    "2. **`filter`:** Esta transformación devuelve un nuevo RDD formado por los elementos para los cuales la función proporcionada devuelve `True`. Aquí, hemos filtrado solo aquellos nombres que tienen más de 4 caracteres.\n",
    "   \n",
    "El resultado final muestra los nombres en mayúsculas y los nombres filtrados con más de 4 caracteres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d42b46-7298-4137-9640-95a0788f510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['ALICE', 'BOB', 'CHARLIE', 'DAVID', 'EVA'], ['ALICE', 'CHARLIE', 'DAVID'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformación: map\n",
    "rdd_uppercase = rdd_from_list.map(lambda name: name.upper())\n",
    "\n",
    "# Transformación: filter\n",
    "rdd_filtered = rdd_uppercase.filter(lambda name: len(name) > 4)\n",
    "\n",
    "# Mostrando resultados\n",
    "rdd_uppercase.collect(), rdd_filtered.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a4ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5da1a556-3175-41a9-a62e-0eab70e67fa3",
   "metadata": {},
   "source": [
    "### Acciones en RDD\n",
    "\n",
    "Las acciones son operaciones que devuelven un valor al programa driver o escriben datos en un sistema de almacenamiento externo. En esta sección, ejecutamos dos acciones comunes: `count` y `first`.\n",
    "\n",
    "1. **`count`:** Esta acción devuelve el número de elementos en el RDD. En nuestro ejemplo, contamos el número de nombres con más de 4 caracteres.\n",
    "2. **`first`:** Esta acción devuelve el primer elemento del RDD. Aquí, recuperamos el primer nombre del RDD filtrado.\n",
    "\n",
    "Las acciones son las que realmente desencadenan la ejecución en Spark. Hasta que se llama a una acción, las transformaciones se evalúan de manera \"perezosa\", lo que significa que no se ejecutan hasta que se necesita el resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c9d309-8ac5-47a3-bd25-9e0f8d12aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'ALICE')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acciones: count y first\n",
    "num_elements = rdd_filtered.count()\n",
    "first_element = rdd_filtered.first()\n",
    "\n",
    "num_elements, first_element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76ef6c-08dd-4f61-ba00-726bb549c829",
   "metadata": {},
   "source": [
    "### Transformación `flatMap`\n",
    "\n",
    "La transformación `flatMap` es similar a `map`, pero cada entrada puede ser mapeada a cero o más salidas. En otras palabras, mientras que `map` produce un nuevo RDD con el mismo número de elementos que el original, `flatMap` puede producir un RDD con un número diferente de elementos.\n",
    "\n",
    "En el ejemplo:\n",
    "1. Creamos un RDD a partir de una lista de oraciones.\n",
    "2. Usamos `flatMap` para dividir cada oración en palabras, lo que resulta en un RDD donde cada elemento es una palabra individual.\n",
    "3. La acción `collect` se utiliza para visualizar el resultado.\n",
    "\n",
    "Con `flatMap`, hemos \"aplanado\" las oraciones en un conjunto de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f872e01-9dc5-475b-9462-6940c8ab43d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', 'I', 'am', 'learning', 'Spark', 'This', 'is', 'fun']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformación: flatMap\n",
    "sentences = [\"Hello world\", \"I am learning Spark\", \"This is fun\"]\n",
    "rdd_sentences = spark.sparkContext.parallelize(sentences)\n",
    "rdd_words = rdd_sentences.flatMap(lambda sentence: sentence.split())\n",
    "\n",
    "# Acción: collect\n",
    "rdd_words.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbdf11e-8714-4d93-adc4-3edc3fd4c4e3",
   "metadata": {},
   "source": [
    "### Acciones `reduce` y `take`\n",
    "\n",
    "1. **`reduce`:** Esta acción toma una función que acepta dos argumentos y devuelve un solo valor, y utiliza esta función para reducir los elementos del RDD a un único valor. En nuestro caso, usamos `reduce` para sumar las longitudes de todas las palabras en el RDD.\n",
    "2. **`take`:** Esta acción devuelve una lista con los primeros `n` elementos del RDD. En el ejemplo, recuperamos las tres primeras palabras del RDD.\n",
    "\n",
    "Estas acciones son útiles para obtener información resumida o subconjuntos de datos del RDD para su análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbdb89b-2f0b-4aaf-8154-77d058ac6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, ['Hello', 'world', 'I'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acciones: reduce y take\n",
    "sum_of_lengths = rdd_words.map(lambda word: len(word)).reduce(lambda a, b: a + b)\n",
    "first_three_words = rdd_words.take(3)\n",
    "\n",
    "sum_of_lengths, first_three_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0974982-b8e7-41bd-a3de-2ec9df3e46e0",
   "metadata": {},
   "source": [
    "### Contar la Frecuencia de Palabras\n",
    "\n",
    "El contar palabras es uno de los ejemplos canónicos en el procesamiento de datos distribuidos. En este ejemplo, contamos la frecuencia de palabras en un conjunto de textos utilizando RDDs.\n",
    "\n",
    "1. **`flatMap`:** Primero, dividimos cada frase en palabras individuales y las convertimos a minúsculas para asegurarnos de que \"Spark\" y \"spark\" sean tratados como la misma palabra.\n",
    "2. **`map`:** Luego, transformamos el RDD de palabras en un RDD de pares, donde cada palabra está asociada al número 1. Estos pares nos ayudarán a contar las ocurrencias de cada palabra.\n",
    "3. **`reduceByKey`:** Esta transformación agrupa el RDD por palabras y suma sus valores para obtener la frecuencia de cada palabra.\n",
    "\n",
    "El resultado final es un RDD de pares (palabra, frecuencia) que muestra cuántas veces aparece cada palabra en el conjunto de textos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88f612d-8c9d-4e5d-8c33-a0913dc5d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('i', 1),\n",
       " ('why', 1),\n",
       " ('hadoop', 1),\n",
       " ('spark', 5),\n",
       " ('popular?', 1),\n",
       " ('versus', 1),\n",
       " ('fast', 1),\n",
       " ('is', 3),\n",
       " ('awesome', 1),\n",
       " ('love', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrada: Un RDD con varias frases\n",
    "text_data = [\"Spark is awesome\", \n",
    "             \"Spark is fast\", \n",
    "             \"I love Spark\", \n",
    "             \"Why is Spark popular?\", \n",
    "             \"Spark versus Hadoop\"]\n",
    "\n",
    "rdd_text = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Dividimos el texto en palabras y lo ponemos en minúsculas\n",
    "words = rdd_text.flatMap(lambda sentence: sentence.lower().split())\n",
    "\n",
    "# Creamos un par RDD con cada palabra y el número 1\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reducimos por clave para contar las ocurrencias de cada palabra\n",
    "word_count = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Resultado\n",
    "word_count.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7354e-01e2-4724-9a1e-9e5537d0082a",
   "metadata": {},
   "source": [
    "### Operaciones de Conjunto en RDD\n",
    "\n",
    "Operaciones de Set\n",
    "\n",
    "Dado que un RDD es una colección de elementos, Spark admite operaciones de conjuntos tradicionales en RDDs. Estas operaciones son especialmente útiles cuando trabajamos con datos en los que queremos realizar uniones, intersecciones o diferencias.\n",
    "\n",
    "\n",
    "\n",
    "1. **`union`:** Combina los elementos de dos RDDs. Si hay elementos duplicados, estos se mantienen.\n",
    "2. **`intersection`:** Devuelve solo los elementos que están presentes en ambos RDDs.\n",
    "\n",
    "En nuestro ejemplo, hemos creado dos RDDs con nombres de frutas. A través de `union`, obtenemos un RDD con todas las frutas de ambos RDDs. Con `intersection`, obtenemos un RDD con solo las frutas que aparecen en ambos RDDs originales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c196592c-7004-4130-9cc3-717d22860421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['apple', 'banana', 'cherry', 'banana', 'cherry', 'date', 'fig'],\n",
       " ['banana', 'cherry'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([\"apple\", \"banana\", \"cherry\"])\n",
    "rdd2 = spark.sparkContext.parallelize([\"banana\", \"cherry\", \"date\", \"fig\"])\n",
    "\n",
    "# Unión\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "\n",
    "# Intersección\n",
    "intersection_rdd = rdd1.intersection(rdd2)\n",
    "\n",
    "union_rdd.collect(), intersection_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a166db-61c2-41e1-bb69-3861df949f13",
   "metadata": {},
   "source": [
    "### Particionamiento en RDD\n",
    "\n",
    "El particionamiento es un proceso clave en Spark que determina cómo se distribuyen los datos en los clústeres. Un particionamiento adecuado puede mejorar significativamente la eficiencia de las operaciones en Spark.\n",
    "\n",
    "El particionamiento controla cómo se distribuyen los datos físicamente a través de los nodos del clúster al almacenar un RDD o DataFrame.\n",
    "\n",
    "1. **`getNumPartitions`:** Nos dice cuántas particiones tiene un RDD.\n",
    "2. **`repartition`:** Nos permite cambiar el número de particiones. Es útil cuando se sabe que se va a trabajar con un gran volumen de datos y se quiere optimizar la distribución de esos datos.\n",
    "\n",
    "En el ejemplo, primero verificamos el número de particiones del RDD original y luego reparticionamos el RDD para que tenga 2 particiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778a2388-1497-40fe-9b47-980ca26855b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de particiones del RDD original\n",
    "num_partitions = rdd_text.getNumPartitions()\n",
    "\n",
    "# Reparticionando el RDD a 2 particiones\n",
    "repartitioned_rdd = rdd_text.repartition(2)\n",
    "\n",
    "# Número de particiones después de reparticionar\n",
    "new_num_partitions = repartitioned_rdd.getNumPartitions()\n",
    "\n",
    "num_partitions, new_num_partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62dd91-9194-4d6a-a157-4dac450901a6",
   "metadata": {},
   "source": [
    "### Persistencia en RDDs\n",
    "\n",
    "Uno de los principales atractivos de Spark es su capacidad para almacenar datos en memoria entre operaciones, lo que permite realizar cálculos rápidos. Esta característica se denomina \"persistencia\" en Spark.\n",
    "\n",
    "Cuando trabajamos con conjuntos de datos en los que realizamos múltiples operaciones (como filtrar, mapear, etc.), puede ser útil guardar el RDD en memoria, especialmente si vamos a reutilizar esos datos. Esto evita tener que recomputar todo desde el principio cada vez.\n",
    "\n",
    "Niveles de Persistencia\n",
    "\n",
    "Spark ofrece varios niveles de persistencia:\n",
    "\n",
    "MEMORY_ONLY: Almacena el RDD solo en memoria.\n",
    "\n",
    "MEMORY_AND_DISK: Almacena el RDD en memoria, pero si no cabe, guarda las particiones que no caben en el disco.\n",
    "\n",
    "MEMORY_ONLY_SER: Similar a MEMORY_ONLY, pero con los datos serializados, lo que puede ser más eficiente en términos de espacio pero más lento en términos de acceso.\n",
    "\n",
    "MEMORY_AND_DISK_SER: Combina las características de MEMORY_AND_DISK y MEMORY_ONLY_SER.\n",
    "\n",
    "DISK_ONLY: Almacena el RDD solo en disco.\n",
    "\n",
    "Es importante elegir el nivel de persistencia adecuado según las necesidades específicas del análisis y los recursos disponibles.\n",
    "\n",
    "\n",
    "La persistencia permite almacenar un RDD en memoria o en disco, lo que facilita su reutilización en operaciones posteriores sin tener que recomputarlo.\n",
    "\n",
    "1. **`persist()`:** Este método permite almacenar un RDD en memoria o en disco. Hay varios niveles de almacenamiento, siendo `StorageLevel.MEMORY_ONLY` uno de los más comunes, que guarda el RDD solo en memoria.\n",
    "2. **`is_cached`:** Es una propiedad que verifica si un RDD está actualmente almacenado en memoria o no.\n",
    "\n",
    "En el ejemplo, persistimos el RDD `rdd_text` en memoria y luego realizamos algunas operaciones sobre él. Finalmente, comprobamos si el RDD está realmente almacenado en memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d3800f-b2db-4625-a9c7-b87a57068386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Persistir el RDD en memoria\n",
    "rdd_text.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Realizamos algunas operaciones\n",
    "count_words = rdd_text.flatMap(lambda sentence: sentence.split()).count()\n",
    "\n",
    "# Verificar si el RDD está persistido\n",
    "is_persisted = rdd_text.is_cached\n",
    "\n",
    "count_words, is_persisted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35e29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
