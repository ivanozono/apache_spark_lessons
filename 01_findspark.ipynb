{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d3a30c-2a69-4a11-a442-6f758a4e6b64",
   "metadata": {},
   "source": [
    "\n",
    "## Inicialización de PySpark en Jupyter Notebook\n",
    "\n",
    "Para usar PySpark dentro de Jupyter Notebook, primero es necesario localizar e inicializar la instalación de Spark en tu máquina. Esto se puede lograr utilizando el paquete `findspark`.\n",
    "\n",
    "### Código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ca9b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/ivanozono/Proyects/spark_leccions/env/bin/pip: bad interpreter: /Users/ivanozono/Proyects/spark_juegos_olimpicos/env/bin/python3: no such file or directory\n",
      "Requirement already satisfied: findspark in /Users/ivanozono/anaconda3/lib/python3.10/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6f1d1-5b12-4cd4-a080-5693ec6cd7fe",
   "metadata": {},
   "source": [
    "## Instalación del Paquete `findspark`\n",
    "\n",
    "El paquete `findspark` es esencial cuando trabajas con PySpark en entornos como Jupyter Notebook. Permite inicializar Spark de manera que pueda ser usado dentro de estos entornos.\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "- **`!pip install findspark`**:\n",
    "    - Este comando se utiliza para instalar el paquete `findspark` utilizando el administrador de paquetes de Python, `pip`.\n",
    "    ```python\n",
    "    !pip install findspark\n",
    "    ```\n",
    "    - El signo de exclamación (`!`) al inicio del comando indica que se está ejecutando un comando del sistema desde dentro de Jupyter Notebook. En este caso, estamos ejecutando un comando `pip`.\n",
    "    - `pip` es el administrador de paquetes predeterminado para Python. Se utiliza para instalar y administrar paquetes adicionales que no están incluidos en la biblioteca estándar de Python.\n",
    "    - `install findspark`: Esta es la instrucción que le dice a `pip` que instale el paquete `findspark`.\n",
    "\n",
    "Una vez que se ha ejecutado este comando, el paquete `findspark` estará disponible en tu entorno y podrás importarlo y usarlo en tu código para inicializar y trabajar con PySpark en Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb6d962-a509-4626-a63a-a209485bdcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/21 16:10:44 WARN Utils: Your hostname, MacBook-Air-de-Ivan.local resolves to a loopback address: 127.0.0.1; using 192.168.0.2 instead (on interface en0)\n",
      "23/09/21 16:10:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/21 16:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/21 16:10:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/21 16:10:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkInJupyter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f7cc4-25a6-4ace-b033-fd2d2d1e40e2",
   "metadata": {},
   "source": [
    "## Inicialización y Configuración de PySpark en Jupyter Notebook\n",
    "\n",
    "Para trabajar con PySpark dentro de Jupyter Notebook, es esencial inicializar y configurar correctamente la sesión de Spark. En el código proporcionado, se demuestra cómo hacerlo.\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "- **Inicialización de Spark**:\n",
    "    - Antes de usar PySpark, debemos localizar e inicializar la instalación de Spark en la máquina. Esto se hace con la ayuda del paquete `findspark`.\n",
    "    ```python\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    ```\n",
    "    - `import findspark`: Importamos el módulo `findspark`.\n",
    "    - `findspark.init()`: Con este método, localizamos e inicializamos la instalación de Spark, permitiendo que Jupyter Notebook acceda a las funcionalidades de Spark.\n",
    "\n",
    "- **Creación de la Sesión de Spark**:\n",
    "    - Una vez inicializado Spark, es necesario crear una sesión para trabajar con él. La sesión se crea y configura usando la clase `SparkSession`.\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkInJupyter\").getOrCreate()\n",
    "    ```\n",
    "    - `from pyspark.sql import SparkSession`: Importamos la clase `SparkSession`, que es el punto de entrada para cualquier funcionalidad relacionada con DataFrame en Spark.\n",
    "    - `spark = SparkSession.builder`: Accedemos al constructor de `SparkSession`.\n",
    "    - `.master(\"local[*]\")`: Configuramos Spark para que se ejecute localmente en la máquina usando todos los núcleos disponibles. Aquí, `[*]` indica que se deben usar todos los núcleos.\n",
    "    - `.appName(\"SparkInJupyter\")`: Definimos el nombre de la aplicación Spark para identificación en el Spark UI.\n",
    "    - `.getOrCreate()`: Finalmente, creamos una nueva sesión Spark si no existe ninguna o recuperamos una ya existente.\n",
    "\n",
    "Una vez ejecutado este código, tendrás una instancia activa de Spark, llamada `spark`, lista para trabajar con DataFrames y otras características de PySpark directamente en Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd666950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346e8d4-fe01-446d-addb-0b139c6d6231",
   "metadata": {},
   "source": [
    "## Creación y Visualización de un DataFrame en PySpark\n",
    "\n",
    "En el código proporcionado, estamos creando un DataFrame en PySpark utilizando datos de ejemplo y luego mostrando su contenido.\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "- **`data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]`**:\n",
    "    - Aquí, estamos definiendo una lista de tuplas. Cada tupla representa una fila del DataFrame que queremos crear. La primera entrada de cada tupla es una cadena que representa un nombre, y la segunda entrada es un número entero.\n",
    "\n",
    "- **`df = spark.createDataFrame(data, [\"name\", \"value\"])`**:\n",
    "    - Usamos el método `createDataFrame` del objeto `spark` (que es una instancia de `SparkSession`) para crear un DataFrame.\n",
    "    - El primer argumento (`data`) es la lista de tuplas que definimos anteriormente.\n",
    "    - El segundo argumento (`[\"name\", \"value\"]`) es una lista de cadenas que define los nombres de las columnas del DataFrame.\n",
    "\n",
    "- **`df.show()`**:\n",
    "    - Este método muestra las primeras filas del DataFrame en una tabla. Por defecto, muestra las primeras 20 filas, pero puedes pasar un número específico como argumento si deseas ver un número diferente de filas.\n",
    "    - Es útil para obtener una vista rápida del contenido del DataFrame después de crearlo o realizar operaciones sobre él.\n",
    "\n",
    "Después de ejecutar el código, deberías ver una tabla con tres filas y dos columnas, mostrando los datos que definimos en la lista `data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2153263",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b375baf-4030-4785-8388-fe0e5c17edbc",
   "metadata": {},
   "source": [
    "## Detener la Sesión de Spark\n",
    "\n",
    "En PySpark, es importante liberar recursos cuando ya no se necesitan. Una de las formas de hacerlo es deteniendo la sesión de Spark que se está utilizando.\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "- **`spark.stop()`**:\n",
    "    - Este método se utiliza para detener una sesión de Spark en ejecución.\n",
    "    ```python\n",
    "    spark.stop()\n",
    "    ```\n",
    "    - Al ejecutar `spark.stop()`, se liberan todos los recursos asociados con la sesión de Spark actual. Esto incluye la terminación de cualquier tarea en ejecución y la liberación de memoria y otros recursos utilizados por la sesión.\n",
    "    - Es una buena práctica detener la sesión de Spark cuando hayas terminado de trabajar con ella, especialmente si estás trabajando en un entorno con recursos limitados o compartidos.\n",
    "\n",
    "Después de ejecutar este código, la instancia de Spark que estabas utilizando será detenida, y deberás crear una nueva sesión si deseas continuar trabajando con PySpark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
